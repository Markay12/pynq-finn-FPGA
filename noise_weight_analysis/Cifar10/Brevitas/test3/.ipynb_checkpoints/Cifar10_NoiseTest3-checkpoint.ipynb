{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8d2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "from unicodedata import decimal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# add imports for randomness\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "# Brevitas imports\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.quant import Int32Bias\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For adaptive learning rate import\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "## Imports from utils file for my defined noise functions\n",
    "import sys\n",
    "sys.path.append('C:/Users/ashin/source/repos/Cifar10_Pytorch_NoiseAnalysis/Cifar10_Pytorch_NoiseAnalysis/pynq-finn-FPGA/noise_weight_analysis/utils/')\n",
    "\n",
    "from noise_functions import random_clust_mask, add_mask_to_model_brevitas, mask_noise_plots_brevitas, add_digital_noise, add_digital_noise_to_model_brevitas, ber_noise_plot_brevitas, add_gaussian_noise, add_gaussian_noise_to_model_brevitas, gaussian_noise_plots_brevitas, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29bdbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf08b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([128, 3, 32, 32])\n",
      "50000\n",
      "Samples in each set: train = 50000, test = 391\n",
      "Shape of one input sample: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define data augmentation transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "# Use the validation transform for the validation dataset\n",
    "val_set =torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=val_transform)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "a = next(iter(train_loader))\n",
    "print(a[0].size())\n",
    "print(len(train_set))\n",
    "\n",
    "print(\"Samples in each set: train = %d, test = %s\" % (len(train_set), len(train_loader))) \n",
    "print(\"Shape of one input sample: \" +  str(train_set[0][0].shape))\n",
    "\n",
    "## Data Loader\n",
    "#\n",
    "# Using PyTorch dataloader we can create a convenient iterator over the dataset that returns batches of data, rather than requiring manual batch creation.\n",
    "\n",
    "# set batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Create a DataLoader for a training dataset with a batch size of 1000\n",
    "train_quantized_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_quantized_loader = DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d91be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Shape:\n",
      "-------------------------\n",
      "Input shape for 1 batch: torch.Size([128, 3, 32, 32])\n",
      "Label shape for 1 batch: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "print(\"\\nDataset Shape:\\n-------------------------\")\n",
    "for x, y in train_loader:\n",
    "    print(\"Input shape for 1 batch: \" + str(x.shape))\n",
    "    print(\"Label shape for 1 batch: \" + str(y.shape))\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a33c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer1 = qnn.QuantConv2d(3, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer2 = qnn.QuantConv2d(32, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer3 = qnn.QuantConv2d(32, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer4 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer5 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu5 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(64 * 8 * 8, 512, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu6 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc2 = qnn.QuantLinear(512, 10, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant_inp(x)\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.relu2(self.layer2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu3(self.layer3(x))\n",
    "        x = self.relu4(self.layer4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu5(self.layer5(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu6(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66bec354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import testing\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Initialize the model, optimizer, and criterion\n",
    "model = CIFAR10CNN().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 80\n",
    "best_test_accuracy = 0\n",
    "patience = 8\n",
    "no_improvement_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a8babc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/391], Loss: 1.6589\n",
      "Epoch [1/80], Step [200/391], Loss: 1.6362\n",
      "Epoch [1/80], Step [300/391], Loss: 1.4192\n",
      "Epoch [1/80], Test Accuracy: 49.77%, Precision: 0.50, Recall: 0.50, F1 score: 0.49\n",
      "Epoch [2/80], Step [100/391], Loss: 1.3538\n",
      "Epoch [2/80], Step [200/391], Loss: 1.3180\n",
      "Epoch [2/80], Step [300/391], Loss: 1.3583\n",
      "Epoch [2/80], Test Accuracy: 58.86%, Precision: 0.60, Recall: 0.59, F1 score: 0.58\n",
      "Epoch [3/80], Step [100/391], Loss: 1.1688\n",
      "Epoch [3/80], Step [200/391], Loss: 1.0445\n",
      "Epoch [3/80], Step [300/391], Loss: 1.0118\n",
      "Epoch [3/80], Test Accuracy: 63.20%, Precision: 0.64, Recall: 0.63, F1 score: 0.63\n",
      "Epoch [4/80], Step [100/391], Loss: 1.0541\n",
      "Epoch [4/80], Step [200/391], Loss: 0.7828\n",
      "Epoch [4/80], Step [300/391], Loss: 0.9463\n",
      "Epoch [4/80], Test Accuracy: 67.58%, Precision: 0.68, Recall: 0.68, F1 score: 0.67\n",
      "Epoch [5/80], Step [100/391], Loss: 0.8469\n",
      "Epoch [5/80], Step [200/391], Loss: 1.0458\n",
      "Epoch [5/80], Step [300/391], Loss: 0.9381\n",
      "Epoch [5/80], Test Accuracy: 70.22%, Precision: 0.71, Recall: 0.70, F1 score: 0.70\n",
      "Epoch [6/80], Step [100/391], Loss: 0.8126\n",
      "Epoch [6/80], Step [200/391], Loss: 0.8148\n",
      "Epoch [6/80], Step [300/391], Loss: 0.9277\n",
      "Epoch [6/80], Test Accuracy: 73.83%, Precision: 0.75, Recall: 0.74, F1 score: 0.74\n",
      "Epoch [7/80], Step [100/391], Loss: 0.7094\n",
      "Epoch [7/80], Step [200/391], Loss: 0.6735\n",
      "Epoch [7/80], Step [300/391], Loss: 0.7567\n",
      "Epoch [7/80], Test Accuracy: 75.00%, Precision: 0.76, Recall: 0.75, F1 score: 0.75\n",
      "Epoch [8/80], Step [100/391], Loss: 0.8030\n",
      "Epoch [8/80], Step [200/391], Loss: 0.7156\n",
      "Epoch [8/80], Step [300/391], Loss: 0.7072\n",
      "Epoch [8/80], Test Accuracy: 75.49%, Precision: 0.76, Recall: 0.75, F1 score: 0.75\n",
      "Epoch [9/80], Step [100/391], Loss: 0.6180\n",
      "Epoch [9/80], Step [200/391], Loss: 0.6346\n",
      "Epoch [9/80], Step [300/391], Loss: 0.7064\n",
      "Epoch [9/80], Test Accuracy: 77.24%, Precision: 0.77, Recall: 0.77, F1 score: 0.77\n",
      "Epoch [10/80], Step [100/391], Loss: 0.5839\n",
      "Epoch [10/80], Step [200/391], Loss: 0.5653\n",
      "Epoch [10/80], Step [300/391], Loss: 0.6655\n",
      "Epoch [10/80], Test Accuracy: 77.83%, Precision: 0.78, Recall: 0.78, F1 score: 0.78\n",
      "Epoch [11/80], Step [100/391], Loss: 0.8056\n",
      "Epoch [11/80], Step [200/391], Loss: 0.4408\n",
      "Epoch [11/80], Step [300/391], Loss: 0.6801\n",
      "Epoch [11/80], Test Accuracy: 77.37%, Precision: 0.78, Recall: 0.77, F1 score: 0.77\n",
      "Epoch [12/80], Step [100/391], Loss: 0.7703\n",
      "Epoch [12/80], Step [200/391], Loss: 0.6337\n",
      "Epoch [12/80], Step [300/391], Loss: 0.4865\n",
      "Epoch [12/80], Test Accuracy: 79.90%, Precision: 0.80, Recall: 0.80, F1 score: 0.80\n",
      "Epoch [13/80], Step [100/391], Loss: 0.7313\n",
      "Epoch [13/80], Step [200/391], Loss: 0.6901\n",
      "Epoch [13/80], Step [300/391], Loss: 0.6021\n",
      "Epoch [13/80], Test Accuracy: 78.48%, Precision: 0.79, Recall: 0.78, F1 score: 0.79\n",
      "Epoch [14/80], Step [100/391], Loss: 0.5715\n",
      "Epoch [14/80], Step [200/391], Loss: 0.5873\n",
      "Epoch [14/80], Step [300/391], Loss: 0.6233\n",
      "Epoch [14/80], Test Accuracy: 79.97%, Precision: 0.80, Recall: 0.80, F1 score: 0.80\n",
      "Epoch [15/80], Step [100/391], Loss: 0.5613\n",
      "Epoch [15/80], Step [200/391], Loss: 0.6593\n",
      "Epoch [15/80], Step [300/391], Loss: 0.5156\n",
      "Epoch [15/80], Test Accuracy: 80.71%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [16/80], Step [100/391], Loss: 0.4751\n",
      "Epoch [16/80], Step [200/391], Loss: 0.5334\n",
      "Epoch [16/80], Step [300/391], Loss: 0.3753\n",
      "Epoch [16/80], Test Accuracy: 80.21%, Precision: 0.81, Recall: 0.80, F1 score: 0.80\n",
      "Epoch [17/80], Step [100/391], Loss: 0.4349\n",
      "Epoch [17/80], Step [200/391], Loss: 0.4663\n",
      "Epoch [17/80], Step [300/391], Loss: 0.4959\n",
      "Epoch [17/80], Test Accuracy: 80.79%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [18/80], Step [100/391], Loss: 0.5515\n",
      "Epoch [18/80], Step [200/391], Loss: 0.4793\n",
      "Epoch [18/80], Step [300/391], Loss: 0.5683\n",
      "Epoch [18/80], Test Accuracy: 81.68%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [19/80], Step [100/391], Loss: 0.3946\n",
      "Epoch [19/80], Step [200/391], Loss: 0.5767\n",
      "Epoch [19/80], Step [300/391], Loss: 0.5016\n",
      "Epoch [19/80], Test Accuracy: 80.71%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [20/80], Step [100/391], Loss: 0.4341\n",
      "Epoch [20/80], Step [200/391], Loss: 0.3447\n",
      "Epoch [20/80], Step [300/391], Loss: 0.5086\n",
      "Epoch [20/80], Test Accuracy: 80.69%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [21/80], Step [100/391], Loss: 0.3728\n",
      "Epoch [21/80], Step [200/391], Loss: 0.3656\n",
      "Epoch [21/80], Step [300/391], Loss: 0.4590\n",
      "Epoch [21/80], Test Accuracy: 81.31%, Precision: 0.82, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [22/80], Step [100/391], Loss: 0.4560\n",
      "Epoch [22/80], Step [200/391], Loss: 0.5087\n",
      "Epoch [22/80], Step [300/391], Loss: 0.4933\n",
      "Epoch [22/80], Test Accuracy: 82.46%, Precision: 0.83, Recall: 0.82, F1 score: 0.83\n",
      "Epoch [23/80], Step [100/391], Loss: 0.5437\n",
      "Epoch [23/80], Step [200/391], Loss: 0.4950\n",
      "Epoch [23/80], Step [300/391], Loss: 0.4699\n",
      "Epoch [23/80], Test Accuracy: 80.83%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [24/80], Step [100/391], Loss: 0.4180\n",
      "Epoch [24/80], Step [200/391], Loss: 0.6047\n",
      "Epoch [24/80], Step [300/391], Loss: 0.3259\n",
      "Epoch [24/80], Test Accuracy: 82.66%, Precision: 0.83, Recall: 0.83, F1 score: 0.82\n",
      "Epoch [25/80], Step [100/391], Loss: 0.4896\n",
      "Epoch [25/80], Step [200/391], Loss: 0.5832\n",
      "Epoch [25/80], Step [300/391], Loss: 0.4622\n",
      "Epoch [25/80], Test Accuracy: 82.79%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [26/80], Step [100/391], Loss: 0.4405\n",
      "Epoch [26/80], Step [200/391], Loss: 0.6092\n",
      "Epoch [26/80], Step [300/391], Loss: 0.4196\n",
      "Epoch [26/80], Test Accuracy: 82.57%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [27/80], Step [100/391], Loss: 0.5075\n",
      "Epoch [27/80], Step [200/391], Loss: 0.5523\n",
      "Epoch [27/80], Step [300/391], Loss: 0.5876\n",
      "Epoch [27/80], Test Accuracy: 82.98%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [28/80], Step [100/391], Loss: 0.4863\n",
      "Epoch [28/80], Step [200/391], Loss: 0.3804\n",
      "Epoch [28/80], Step [300/391], Loss: 0.3231\n",
      "Epoch [28/80], Test Accuracy: 82.94%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [29/80], Step [100/391], Loss: 0.4799\n",
      "Epoch [29/80], Step [200/391], Loss: 0.4336\n",
      "Epoch [29/80], Step [300/391], Loss: 0.4009\n",
      "Epoch [29/80], Test Accuracy: 82.34%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [30/80], Step [100/391], Loss: 0.3959\n",
      "Epoch [30/80], Step [200/391], Loss: 0.4662\n",
      "Epoch [30/80], Step [300/391], Loss: 0.4038\n",
      "Epoch [30/80], Test Accuracy: 82.35%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [31/80], Step [100/391], Loss: 0.3194\n",
      "Epoch [31/80], Step [200/391], Loss: 0.4248\n",
      "Epoch [31/80], Step [300/391], Loss: 0.4052\n",
      "Epoch [31/80], Test Accuracy: 83.39%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [32/80], Step [100/391], Loss: 0.4379\n",
      "Epoch [32/80], Step [200/391], Loss: 0.3314\n",
      "Epoch [32/80], Step [300/391], Loss: 0.3874\n",
      "Epoch [32/80], Test Accuracy: 82.56%, Precision: 0.83, Recall: 0.83, F1 score: 0.82\n",
      "Epoch [33/80], Step [100/391], Loss: 0.4039\n",
      "Epoch [33/80], Step [200/391], Loss: 0.3039\n",
      "Epoch [33/80], Step [300/391], Loss: 0.3625\n",
      "Epoch [33/80], Test Accuracy: 82.54%, Precision: 0.83, Recall: 0.83, F1 score: 0.82\n",
      "Epoch [34/80], Step [100/391], Loss: 0.2694\n",
      "Epoch [34/80], Step [200/391], Loss: 0.3687\n",
      "Epoch [34/80], Step [300/391], Loss: 0.4758\n",
      "Epoch [34/80], Test Accuracy: 82.51%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [35/80], Step [100/391], Loss: 0.4118\n",
      "Epoch [35/80], Step [200/391], Loss: 0.4923\n",
      "Epoch [35/80], Step [300/391], Loss: 0.4523\n",
      "Epoch [35/80], Test Accuracy: 82.99%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [36/80], Step [100/391], Loss: 0.3442\n",
      "Epoch [36/80], Step [200/391], Loss: 0.4312\n",
      "Epoch [36/80], Step [300/391], Loss: 0.3464\n",
      "Epoch [36/80], Test Accuracy: 82.68%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [37/80], Step [100/391], Loss: 0.3469\n",
      "Epoch [37/80], Step [200/391], Loss: 0.4024\n",
      "Epoch [37/80], Step [300/391], Loss: 0.5902\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [37/80], Test Accuracy: 82.28%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [38/80], Step [100/391], Loss: 0.3163\n",
      "Epoch [38/80], Step [200/391], Loss: 0.3610\n",
      "Epoch [38/80], Step [300/391], Loss: 0.4059\n",
      "Epoch [38/80], Test Accuracy: 84.90%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/80], Step [100/391], Loss: 0.3271\n",
      "Epoch [39/80], Step [200/391], Loss: 0.3271\n",
      "Epoch [39/80], Step [300/391], Loss: 0.2167\n",
      "Epoch [39/80], Test Accuracy: 84.91%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [40/80], Step [100/391], Loss: 0.2513\n",
      "Epoch [40/80], Step [200/391], Loss: 0.2719\n",
      "Epoch [40/80], Step [300/391], Loss: 0.2433\n",
      "Epoch [40/80], Test Accuracy: 84.89%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [41/80], Step [100/391], Loss: 0.3585\n",
      "Epoch [41/80], Step [200/391], Loss: 0.3375\n",
      "Epoch [41/80], Step [300/391], Loss: 0.3184\n",
      "Epoch [41/80], Test Accuracy: 85.27%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [42/80], Step [100/391], Loss: 0.3382\n",
      "Epoch [42/80], Step [200/391], Loss: 0.3540\n",
      "Epoch [42/80], Step [300/391], Loss: 0.5220\n",
      "Epoch [42/80], Test Accuracy: 85.03%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [43/80], Step [100/391], Loss: 0.3500\n",
      "Epoch [43/80], Step [200/391], Loss: 0.2108\n",
      "Epoch [43/80], Step [300/391], Loss: 0.4878\n",
      "Epoch [43/80], Test Accuracy: 85.15%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [44/80], Step [100/391], Loss: 0.2301\n",
      "Epoch [44/80], Step [200/391], Loss: 0.2517\n",
      "Epoch [44/80], Step [300/391], Loss: 0.2333\n",
      "Epoch [44/80], Test Accuracy: 85.19%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [45/80], Step [100/391], Loss: 0.2206\n",
      "Epoch [45/80], Step [200/391], Loss: 0.3008\n",
      "Epoch [45/80], Step [300/391], Loss: 0.2434\n",
      "Epoch [45/80], Test Accuracy: 85.29%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [46/80], Step [100/391], Loss: 0.2691\n",
      "Epoch [46/80], Step [200/391], Loss: 0.1611\n",
      "Epoch [46/80], Step [300/391], Loss: 0.1889\n",
      "Epoch [46/80], Test Accuracy: 85.29%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [47/80], Step [100/391], Loss: 0.2140\n",
      "Epoch [47/80], Step [200/391], Loss: 0.2869\n",
      "Epoch [47/80], Step [300/391], Loss: 0.2429\n",
      "Epoch [47/80], Test Accuracy: 85.37%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [48/80], Step [100/391], Loss: 0.2738\n",
      "Epoch [48/80], Step [200/391], Loss: 0.2291\n",
      "Epoch [48/80], Step [300/391], Loss: 0.2158\n",
      "Epoch [48/80], Test Accuracy: 85.54%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [49/80], Step [100/391], Loss: 0.2885\n",
      "Epoch [49/80], Step [200/391], Loss: 0.3271\n",
      "Epoch [49/80], Step [300/391], Loss: 0.2895\n",
      "Epoch [49/80], Test Accuracy: 85.28%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [50/80], Step [100/391], Loss: 0.2345\n",
      "Epoch [50/80], Step [200/391], Loss: 0.3732\n",
      "Epoch [50/80], Step [300/391], Loss: 0.2332\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [50/80], Test Accuracy: 85.27%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [51/80], Step [100/391], Loss: 0.1354\n",
      "Epoch [51/80], Step [200/391], Loss: 0.2604\n",
      "Epoch [51/80], Step [300/391], Loss: 0.2003\n",
      "Epoch [51/80], Test Accuracy: 85.69%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [52/80], Step [100/391], Loss: 0.1415\n",
      "Epoch [52/80], Step [200/391], Loss: 0.2858\n",
      "Epoch [52/80], Step [300/391], Loss: 0.1227\n",
      "Epoch [52/80], Test Accuracy: 85.58%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [53/80], Step [100/391], Loss: 0.2468\n",
      "Epoch [53/80], Step [200/391], Loss: 0.3308\n",
      "Epoch [53/80], Step [300/391], Loss: 0.2196\n",
      "Epoch [53/80], Test Accuracy: 85.73%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [54/80], Step [100/391], Loss: 0.3149\n",
      "Epoch [54/80], Step [200/391], Loss: 0.1439\n",
      "Epoch [54/80], Step [300/391], Loss: 0.1788\n",
      "Epoch [54/80], Test Accuracy: 85.82%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [55/80], Step [100/391], Loss: 0.1525\n",
      "Epoch [55/80], Step [200/391], Loss: 0.1675\n",
      "Epoch [55/80], Step [300/391], Loss: 0.2986\n",
      "Epoch [55/80], Test Accuracy: 85.61%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [56/80], Step [100/391], Loss: 0.3711\n",
      "Epoch [56/80], Step [200/391], Loss: 0.1838\n",
      "Epoch [56/80], Step [300/391], Loss: 0.1834\n",
      "Epoch [56/80], Test Accuracy: 85.85%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [57/80], Step [100/391], Loss: 0.1651\n",
      "Epoch [57/80], Step [200/391], Loss: 0.2777\n",
      "Epoch [57/80], Step [300/391], Loss: 0.1439\n",
      "Epoch [57/80], Test Accuracy: 85.70%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [58/80], Step [100/391], Loss: 0.1587\n",
      "Epoch [58/80], Step [200/391], Loss: 0.1597\n",
      "Epoch [58/80], Step [300/391], Loss: 0.2670\n",
      "Epoch 00058: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch [58/80], Test Accuracy: 85.72%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [59/80], Step [100/391], Loss: 0.2611\n",
      "Epoch [59/80], Step [200/391], Loss: 0.1411\n",
      "Epoch [59/80], Step [300/391], Loss: 0.3286\n",
      "Epoch [59/80], Test Accuracy: 85.48%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [60/80], Step [100/391], Loss: 0.1483\n",
      "Epoch [60/80], Step [200/391], Loss: 0.2253\n",
      "Epoch [60/80], Step [300/391], Loss: 0.1935\n",
      "Epoch [60/80], Test Accuracy: 85.70%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [61/80], Step [100/391], Loss: 0.2305\n",
      "Epoch [61/80], Step [200/391], Loss: 0.2745\n",
      "Epoch [61/80], Step [300/391], Loss: 0.2264\n",
      "Epoch [61/80], Test Accuracy: 85.69%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [62/80], Step [100/391], Loss: 0.2489\n",
      "Epoch [62/80], Step [200/391], Loss: 0.2902\n",
      "Epoch [62/80], Step [300/391], Loss: 0.2312\n",
      "Epoch [62/80], Test Accuracy: 85.78%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [63/80], Step [100/391], Loss: 0.2254\n",
      "Epoch [63/80], Step [200/391], Loss: 0.1584\n",
      "Epoch [63/80], Step [300/391], Loss: 0.2254\n",
      "Epoch [63/80], Test Accuracy: 85.77%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [64/80], Step [100/391], Loss: 0.1312\n",
      "Epoch [64/80], Step [200/391], Loss: 0.2541\n",
      "Epoch [64/80], Step [300/391], Loss: 0.2235\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch [64/80], Test Accuracy: 85.87%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [65/80], Step [100/391], Loss: 0.2275\n",
      "Epoch [65/80], Step [200/391], Loss: 0.1896\n",
      "Epoch [65/80], Step [300/391], Loss: 0.1420\n",
      "Epoch [65/80], Test Accuracy: 85.65%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [66/80], Step [100/391], Loss: 0.3147\n",
      "Epoch [66/80], Step [200/391], Loss: 0.2423\n",
      "Epoch [66/80], Step [300/391], Loss: 0.3370\n",
      "Epoch [66/80], Test Accuracy: 85.59%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [67/80], Step [100/391], Loss: 0.2084\n",
      "Epoch [67/80], Step [200/391], Loss: 0.2004\n",
      "Epoch [67/80], Step [300/391], Loss: 0.1936\n",
      "Epoch [67/80], Test Accuracy: 85.45%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [68/80], Step [100/391], Loss: 0.3604\n",
      "Epoch [68/80], Step [200/391], Loss: 0.2692\n",
      "Epoch [68/80], Step [300/391], Loss: 0.1853\n",
      "Epoch [68/80], Test Accuracy: 85.70%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [69/80], Step [100/391], Loss: 0.2380\n",
      "Epoch [69/80], Step [200/391], Loss: 0.1830\n",
      "Epoch [69/80], Step [300/391], Loss: 0.1903\n",
      "Epoch [69/80], Test Accuracy: 85.67%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [70/80], Step [100/391], Loss: 0.2049\n",
      "Epoch [70/80], Step [200/391], Loss: 0.2438\n",
      "Epoch [70/80], Step [300/391], Loss: 0.2043\n",
      "Epoch 00070: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch [70/80], Test Accuracy: 85.65%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [71/80], Step [100/391], Loss: 0.2085\n",
      "Epoch [71/80], Step [200/391], Loss: 0.2042\n",
      "Epoch [71/80], Step [300/391], Loss: 0.2510\n",
      "Epoch [71/80], Test Accuracy: 85.66%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [72/80], Step [100/391], Loss: 0.1257\n",
      "Epoch [72/80], Step [200/391], Loss: 0.2127\n",
      "Epoch [72/80], Step [300/391], Loss: 0.3376\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # training phase\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "    \n",
    "    # Initialize the validation loss\n",
    "    val_loss = 0\n",
    "    \n",
    "    # testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()  # accumulate the validation loss\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "        val_loss /= len(val_loader)  # calculate the average validation loss\n",
    "    \n",
    "        # Update the learning rate using the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        test_accuracy = 100 * correct / total\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), 'test3_model.pth')\n",
    "            no_improvement_counter = 0\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print('Epoch [{}/{}], Test Accuracy: {:.2f}%, Precision: {:.2f}, Recall: {:.2f}, F1 score: {:.2f}'.format(epoch+1, num_epochs, test_accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055fe5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final best test accuracy is: 85.87%\n"
     ]
    }
   ],
   "source": [
    "# Print the best test accuracy\n",
    "print(\"The final best test accuracy is: {:.2f}%\".format(best_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966f95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b94d7e",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d3d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_accuracy > best_test_accuracy:\n",
    "    best_test_accuracy = test_accuracy\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_test_accuracy': best_test_accuracy,\n",
    "        'epoch': epoch\n",
    "        }, 'best_model.pth')\n",
    "    no_improvement_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3573bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of your neural network model\n",
    "model = CIFAR10CNN().to(device)\n",
    "\n",
    "# Load the saved state dictionary from file\n",
    "state_dict = torch.load('best_model.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f829b",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "## Mask Code Here for Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "617632fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_clust_mask(weight_shape, P, gamma):\n",
    "\n",
    "    # Generate random NxN matrix with values between 0 and 1\n",
    "    N = weight_shape[-1]\n",
    "    matrix = np.random.rand(N, N)\n",
    "\n",
    "    # Compute 2D FFT\n",
    "    fft_result = np.fft.fft2(matrix)\n",
    "\n",
    "    # 1D Frequency Vector with N bins\n",
    "    f = np.fft.fftfreq(N, d=1.0/weight_shape[-1])\n",
    "    f_x, f_y = np.meshgrid(f, f)\n",
    "    f_x[0, 0] = 1e-6\n",
    "    f_y[0, 0] = 1e-6\n",
    "\n",
    "    # Create a 2D filter in frequency space that varies inversely with freq over f\n",
    "    # Gamma controls the falloff rate\n",
    "    filter_2D = 1/(np.sqrt(f_x**2 + f_y**2))**gamma\n",
    "\n",
    "    # Mult the 2D elementwise by the filter\n",
    "    filtered_fft = fft_result * filter_2D\n",
    "\n",
    "    # 2D inverse FFT of the filtered result\n",
    "    ifft_result = np.fft.ifft2(filtered_fft)\n",
    "    ifft_result = np.real(ifft_result)\n",
    "\n",
    "    # Set the threshold T equal the the max value in IFFT\n",
    "    T = ifft_result.max()\n",
    "\n",
    "    # Init empty bool mask with same dims as ifft\n",
    "    mask = np.zeros_like(ifft_result, dtype=bool)\n",
    "\n",
    "    decrement_step = 0.01\n",
    "\n",
    "    # Repeat until frac of nonzero values in the mask is greater than or equal to P\n",
    "    while True:\n",
    "        mask = ifft_result > T\n",
    "\n",
    "        current_fraction = np.count_nonzero(mask) / (N * N)\n",
    "\n",
    "        if current_fraction >= P:\n",
    "            break\n",
    "\n",
    "        T -= decrement_step\n",
    "\n",
    "    # Return tensor with the same shape as the input tensor\n",
    "    mask = np.tile(mask, (weight_shape[0], weight_shape[1], 1, 1))\n",
    "    return torch.from_numpy(mask).float().clone().detach()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes a brevitas layer, a mask generated by the random_clust_mask function,\n",
    "and two parameters P and gamma. It generates a random clustered mask using the random_clust_mask\n",
    "function, and then sets the weights at the locations in the mask to zero, effectively sparsifying\n",
    "the layer's weights.\n",
    "\"\"\"\n",
    "\n",
    "def add_mask_to_model_brevitas(model, layer_names, p, gamma, num_perturbations, print_weights=False):\n",
    "\n",
    "    modified_models = []\n",
    "\n",
    "    for _ in range(num_perturbations):\n",
    "\n",
    "        modified_model = deepcopy(model)\n",
    "\n",
    "        for layer_name in layer_names:\n",
    "\n",
    "            layer = getattr(modified_model, layer_name)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # get weights of the tensors\n",
    "                weight_tensor = layer.weight.clone().detach()\n",
    "\n",
    "                # generate mask with correct shape\n",
    "                mask = random_clust_mask(weight_tensor.shape, p, gamma)\n",
    "\n",
    "                if print_weights:\n",
    "                    print(\"Weights before masking:\")\n",
    "                    print(weight_tensor)\n",
    "\n",
    "                # apply mask to the whole weight tensor\n",
    "                mask_tensor = torch.tensor(mask, dtype=torch.float).to(device)\n",
    "                weight_tensor *= mask_tensor\n",
    "\n",
    "                if print_weights:\n",
    "                    print(\"Weights after masking:\")\n",
    "                    print(weight_tensor)\n",
    "\n",
    "                # create new weight parameter and assign to layer\n",
    "                noised_weight = torch.nn.Parameter(weight_tensor, requires_grad=False)\n",
    "                layer.weight = noised_weight\n",
    "\n",
    "        modified_models.append(modified_model)\n",
    "\n",
    "    return modified_models\n",
    "\n",
    "\"\"\"\n",
    "The function mask_noise_plots_brevitas() is a Python function that is used to visualize the effect of different perturbations on the accuracy of a neural network. \n",
    "It generates several visualizations, including a heatmap and a scatter plot.\n",
    "\n",
    "The function takes in the following parameters:\n",
    "\n",
    "    num_perturbations: an integer indicating the number of perturbations to apply to the neural network\n",
    "    layer_names: a list of strings indicating the names of the layers in the neural network\n",
    "    p_vals: a list of floats indicating the p values to use for the mask perturbation\n",
    "    gamma_vals: a list of floats indicating the gamma values to use for the mask perturbation\n",
    "    model: the neural network model to test\n",
    "    device: the device to use for testing the model\n",
    "\n",
    "The function begins by creating a directory for saving the output plots. \n",
    "It then initializes an empty list called all_test_accs to store the test accuracies for each layer.\n",
    "\n",
    "For each layer in layer_names, the function initializes an empty list called test_accs to store \n",
    "the test accuracies for each mask perturbation. The function then iterates over each p and gamma value \n",
    "and adds noise to the model for the defined layer only. It then tests the accuracy of each noisy model \n",
    "and appends the result to the accuracies list. The function then calculates the average accuracy \n",
    "and prints the result.\n",
    "\n",
    "The test accuracies for the current layer are then stored in the all_test_accs list.\n",
    "\n",
    "The function then creates a heatmap for each layer and saves it to disk.\n",
    "It also computes the average test accuracy across all layers for each p and gamma value and creates a\n",
    "heatmap for the average test accuracy. Both the individual and average heatmaps have a color bar \n",
    "indicating the test accuracy.\n",
    "\n",
    "Finally, the function creates a scatter plot showing the test accuracies for each layer at each p \n",
    "and gamma value. The plot has layer_names on the x-axis, p_vals on the y-axis, and gamma_vals on the z-axis. \n",
    "The points in the plot are colored based on the corresponding test accuracy, with a color bar indicating \n",
    "the mapping between color and test accuracy.\n",
    "\"\"\"\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    # testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mask_noise_plots_brevitas(num_perturbations, layer_names, p_values, gamma_values, model, device):\n",
    "    \n",
    "    \n",
    "    for p in range(len(p_values)):\n",
    "        for g in range(len(gamma_values)):\n",
    "            \n",
    "            noisy_models = add_mask_to_model_brevitas(model, layer_names, p_values[p], gamma_values[g], num_perturbations)\n",
    "            \n",
    "            accuracies = []\n",
    "            \n",
    "            for noisy_model in noisy_models:\n",
    "                \n",
    "                noisy_model.to(device)\n",
    "\n",
    "                accuracies.append(test(noisy_model, test_quantized_loader, device))\n",
    "                \n",
    "            avg_accuracy = sum(accuracies)/len(accuracies)\n",
    "            \n",
    "            print(\"P Value: {}\\t Gamma Value: {}\\t Average Accuracy: {}%\".format(p_values[p], gamma_values[g], avg_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b21c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashin\\AppData\\Local\\Temp\\ipykernel_14108\\2427530726.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask_tensor = torch.tensor(mask, dtype=torch.float).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P Value: 0.001\t Gamma Value: 0.001\t Average Accuracy: 10.03%\n",
      "P Value: 0.001\t Gamma Value: 0.24825\t Average Accuracy: 10.0%\n",
      "P Value: 0.001\t Gamma Value: 0.4955\t Average Accuracy: 9.86%\n",
      "P Value: 0.001\t Gamma Value: 0.74275\t Average Accuracy: 10.0%\n",
      "P Value: 0.001\t Gamma Value: 0.99\t Average Accuracy: 10.0%\n",
      "P Value: 0.029500000000000002\t Gamma Value: 0.001\t Average Accuracy: 10.0%\n",
      "P Value: 0.029500000000000002\t Gamma Value: 0.24825\t Average Accuracy: 10.0%\n",
      "P Value: 0.029500000000000002\t Gamma Value: 0.4955\t Average Accuracy: 10.0%\n",
      "P Value: 0.029500000000000002\t Gamma Value: 0.74275\t Average Accuracy: 10.0%\n",
      "P Value: 0.029500000000000002\t Gamma Value: 0.99\t Average Accuracy: 10.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m p_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m      5\u001b[0m gamma_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m0.99\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmask_noise_plots_brevitas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 168\u001b[0m, in \u001b[0;36mmask_noise_plots_brevitas\u001b[1;34m(num_perturbations, layer_names, p_values, gamma_values, model, device)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noisy_model \u001b[38;5;129;01min\u001b[39;00m noisy_models:\n\u001b[0;32m    166\u001b[0m     noisy_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 168\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_quantized_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    170\u001b[0m avg_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(accuracies)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(accuracies)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP Value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Gamma Value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Average Accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p_values[p], gamma_values[g], avg_accuracy))\n",
      "Cell \u001b[1;32mIn[26], line 143\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m    141\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    142\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 143\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    145\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m, in \u001b[0;36mCIFAR10CNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_inp(x)\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\nn\\quant_layer.py:148\u001b[0m, in \u001b[0;36mQuantNonLinearActLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_global_is_quant_layer(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m--> 148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_output(out)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\proxy\\runtime_quant.py:152\u001b[0m, in \u001b[0;36mActQuantProxyFromInjector.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    150\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_handler(y)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfused_activation_quant_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QuantTensor(\u001b[38;5;241m*\u001b[39my, signed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_signed, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\proxy\\runtime_quant.py:83\u001b[0m, in \u001b[0;36mFusedActivationQuantProxy.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_impl(x)\n\u001b[1;32m---> 83\u001b[0m     x, output_scale, output_zp, output_bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, output_scale, output_zp, output_bit_width\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\core\\quant\\int.py:159\u001b[0m, in \u001b[0;36mRescalingIntQuant.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    157\u001b[0m bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsb_clamp_bit_width_impl()\n\u001b[0;32m    158\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_impl(x)\n\u001b[1;32m--> 159\u001b[0m int_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_scaling_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m scale \u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;241m/\u001b[39m int_threshold\n\u001b[0;32m    161\u001b[0m zero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point_impl(x, scale, bit_width)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\core\\scaling\\int_scaling.py:24\u001b[0m, in \u001b[0;36mIntScaling.forward\u001b[1;34m(self, bit_width)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m min_int(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigned, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnarrow_range, bit_width)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmax_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnarrow_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\function\\ops.py:155\u001b[0m, in \u001b[0;36mmax_int\u001b[1;34m(signed, narrow_range, bit_width)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Compute the maximum integer representable by a given number of bits.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    tensor(255)\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m narrow_range:\n\u001b[1;32m--> 155\u001b[0m     value \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbit_width\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signed \u001b[38;5;129;01mand\u001b[39;00m narrow_range:\n\u001b[0;32m    157\u001b[0m     value \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m bit_width) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\_tensor.py:878\u001b[0m, in \u001b[0;36mTensor.__rpow__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rpow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m    877\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mresult_type(other, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Mask\n",
    "layer_names = ['layer1', 'layer2', 'layer3', 'layer4', 'layer5']\n",
    "perturbations = 30\n",
    "p_values = np.linspace(1e-3, 0.4, 15)\n",
    "gamma_values = np.linspace(1e-3, 0.99, 5)\n",
    "\n",
    "\n",
    "mask_noise_plots_brevitas(1, layer_names, p_values, gamma_values, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffa5c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ber_noise_plot_brevitas(num_perturbations, layer_names, ber_vector, model, device):\n",
    "    \n",
    "    if not os.path.exists(\"noise_plots_brevitas/ber_noise/\"):\n",
    "        os.makedirs(\"noise_plots_brevitas/ber_noise/\")\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    all_test_accs = []\n",
    "\n",
    "    for layer in layer_names:\n",
    "        test_accs = []\n",
    "        \n",
    "        for ber in ber_vector:\n",
    "            noisy_models = add_digital_noise_to_model_brevitas(model, [layer], ber, num_perturbations)\n",
    "            \n",
    "            accuracies = []\n",
    "            \n",
    "            for noisy_model in noisy_models:\n",
    "                \n",
    "                noisy_model.to(device)\n",
    "                accuracies.append(test(noisy_model, test_quantized_loader, device))\n",
    "                \n",
    "            avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "            \n",
    "            test_accs.append(avg_accuracy)\n",
    "            \n",
    "            print(\"BER Value: {}\\tAverage Accuracy: {}\".format(ber, avg_accuracy))\n",
    "            \n",
    "        all_test_accs.append(test_accs)\n",
    "        \n",
    "        plt.plot(ber_vector, test_accs,\n",
    "                 label='{} Accuracy at Different Perturbation Levels'.format(layer))\n",
    "        \n",
    "        plt.xlabel('Standard Deviation')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('Effect of Noise on Test Accuracy')\n",
    "        plt.legend()\n",
    "        plt.savefig(\"noise_plots_brevitas/ber_noise/{}.png\".format(layer))\n",
    "        plt.clf()\n",
    "        print('Done with Plot {}'.format(layer))\n",
    "        \n",
    "    avg_test_accs = [sum(x) / len(x) for x in zip(*all_test_accs)]\n",
    "    \n",
    "    plt.plot(ber_vector, avg_test_accs, label='Average',\n",
    "             linewidth=3, linestyle='--', color=\"black\")\n",
    "    \n",
    "    plt.xlabel('BER Value')\n",
    "    \n",
    "    plt.ylabel('Test Accuracy')\n",
    "    \n",
    "    plt.title('Effect of BER Noise on Test Accuracy (Average)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(\"noise_plots_brevitas/ber_noise/average.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "359fd9d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER Value: 1e-06\tAverage Accuracy: 85.88\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.88\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.75899999999999\n",
      "BER Value: 0.001\tAverage Accuracy: 84.695\n",
      "BER Value: 0.01\tAverage Accuracy: 78.97200000000001\n",
      "BER Value: 0.1\tAverage Accuracy: 41.394\n",
      "Done with Plot layer1\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.88\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.88\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.48299999999998\n",
      "BER Value: 0.001\tAverage Accuracy: 84.44500000000001\n",
      "BER Value: 0.01\tAverage Accuracy: 80.296\n",
      "BER Value: 0.1\tAverage Accuracy: 36.361000000000004\n",
      "Done with Plot layer2\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.88\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.736\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.481\n",
      "BER Value: 0.001\tAverage Accuracy: 84.73599999999999\n",
      "BER Value: 0.01\tAverage Accuracy: 82.798\n",
      "BER Value: 0.1\tAverage Accuracy: 62.157999999999994\n",
      "Done with Plot layer3\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.88\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.664\n",
      "BER Value: 0.0001\tAverage Accuracy: 84.992\n",
      "BER Value: 0.001\tAverage Accuracy: 84.72999999999999\n",
      "BER Value: 0.01\tAverage Accuracy: 82.809\n",
      "BER Value: 0.1\tAverage Accuracy: 66.101\n",
      "Done with Plot layer4\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.88\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.672\n",
      "BER Value: 0.0001\tAverage Accuracy: 84.804\n",
      "BER Value: 0.001\tAverage Accuracy: 84.289\n",
      "BER Value: 0.01\tAverage Accuracy: 81.64599999999999\n",
      "BER Value: 0.1\tAverage Accuracy: 62.001999999999995\n",
      "Done with Plot layer5\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.69399999999999\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.508\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.54299999999999\n",
      "BER Value: 0.001\tAverage Accuracy: 85.496\n",
      "BER Value: 0.01\tAverage Accuracy: 84.958\n",
      "BER Value: 0.1\tAverage Accuracy: 79.525\n",
      "Done with Plot fc1\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.88\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.776\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.525\n",
      "BER Value: 0.001\tAverage Accuracy: 84.527\n",
      "BER Value: 0.01\tAverage Accuracy: 80.86\n",
      "BER Value: 0.1\tAverage Accuracy: 62.782000000000004\n",
      "Done with Plot fc2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Digital Noise with BER\n",
    "layer_names = ['layer1', 'layer2', 'layer3', 'layer4', 'layer5', 'fc1', 'fc2']\n",
    "ber_vals = [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1]\n",
    "perturbations = 10\n",
    "\n",
    "ber_noise_plot_brevitas(perturbations, layer_names, ber_vals, model, device, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73d701cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma Value: 0.0, Average Accuracy: 85.88%\n",
      "Sigma Value: 0.02, Average Accuracy: 84.744%\n",
      "Sigma Value: 0.04, Average Accuracy: 81.56300000000002%\n",
      "Sigma Value: 0.06, Average Accuracy: 76.00800000000001%\n",
      "Sigma Value: 0.08, Average Accuracy: 67.448%\n",
      "Sigma Value: 0.1, Average Accuracy: 58.37600000000001%\n",
      "Sigma Value: 0.12, Average Accuracy: 49.99400000000001%\n",
      "Sigma Value: 0.14, Average Accuracy: 41.592%\n",
      "Sigma Value: 0.16, Average Accuracy: 32.379000000000005%\n",
      "Sigma Value: 0.18, Average Accuracy: 24.355%\n",
      "Sigma Value: 0.2, Average Accuracy: 25.327999999999996%\n",
      "Done with Plot layer1\n",
      "Sigma Value: 0.0, Average Accuracy: 85.88%\n",
      "Sigma Value: 0.02, Average Accuracy: 84.56%\n",
      "Sigma Value: 0.04, Average Accuracy: 82.05499999999999%\n",
      "Sigma Value: 0.06, Average Accuracy: 75.004%\n",
      "Sigma Value: 0.08, Average Accuracy: 70.49699999999999%\n",
      "Sigma Value: 0.1, Average Accuracy: 58.277%\n",
      "Sigma Value: 0.12, Average Accuracy: 46.56700000000001%\n",
      "Sigma Value: 0.14, Average Accuracy: 43.016999999999996%\n",
      "Sigma Value: 0.16, Average Accuracy: 37.520999999999994%\n",
      "Sigma Value: 0.18, Average Accuracy: 24.958000000000002%\n",
      "Sigma Value: 0.2, Average Accuracy: 24.227999999999998%\n",
      "Done with Plot layer2\n",
      "Sigma Value: 0.0, Average Accuracy: 85.88%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.22%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.23799999999999%\n",
      "Sigma Value: 0.06, Average Accuracy: 81.977%\n",
      "Sigma Value: 0.08, Average Accuracy: 79.104%\n",
      "Sigma Value: 0.1, Average Accuracy: 73.251%\n",
      "Sigma Value: 0.12, Average Accuracy: 70.961%\n",
      "Sigma Value: 0.14, Average Accuracy: 64.93%\n",
      "Sigma Value: 0.16, Average Accuracy: 58.38199999999999%\n",
      "Sigma Value: 0.18, Average Accuracy: 48.62%\n",
      "Sigma Value: 0.2, Average Accuracy: 46.035000000000004%\n",
      "Done with Plot layer3\n",
      "Sigma Value: 0.0, Average Accuracy: 85.88%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.284%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.6%\n",
      "Sigma Value: 0.06, Average Accuracy: 83.598%\n",
      "Sigma Value: 0.08, Average Accuracy: 81.077%\n",
      "Sigma Value: 0.1, Average Accuracy: 78.806%\n",
      "Sigma Value: 0.12, Average Accuracy: 74.61099999999999%\n",
      "Sigma Value: 0.14, Average Accuracy: 69.44999999999999%\n",
      "Sigma Value: 0.16, Average Accuracy: 66.171%\n",
      "Sigma Value: 0.18, Average Accuracy: 63.327%\n",
      "Sigma Value: 0.2, Average Accuracy: 55.956%\n",
      "Done with Plot layer4\n",
      "Sigma Value: 0.0, Average Accuracy: 85.88%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.29899999999999%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.34599999999999%\n",
      "Sigma Value: 0.06, Average Accuracy: 82.81199999999998%\n",
      "Sigma Value: 0.08, Average Accuracy: 80.217%\n",
      "Sigma Value: 0.1, Average Accuracy: 75.48700000000001%\n",
      "Sigma Value: 0.12, Average Accuracy: 73.58099999999999%\n",
      "Sigma Value: 0.14, Average Accuracy: 70.001%\n",
      "Sigma Value: 0.16, Average Accuracy: 65.193%\n",
      "Sigma Value: 0.18, Average Accuracy: 57.678999999999995%\n",
      "Sigma Value: 0.2, Average Accuracy: 53.36600000000001%\n",
      "Done with Plot layer5\n",
      "Sigma Value: 0.0, Average Accuracy: 85.88%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.63600000000001%\n",
      "Sigma Value: 0.04, Average Accuracy: 85.46399999999998%\n",
      "Sigma Value: 0.06, Average Accuracy: 85.061%\n",
      "Sigma Value: 0.08, Average Accuracy: 84.57600000000001%\n",
      "Sigma Value: 0.1, Average Accuracy: 83.948%\n",
      "Sigma Value: 0.12, Average Accuracy: 83.131%\n",
      "Sigma Value: 0.14, Average Accuracy: 82.25699999999999%\n",
      "Sigma Value: 0.16, Average Accuracy: 81.458%\n",
      "Sigma Value: 0.18, Average Accuracy: 80.02000000000001%\n",
      "Sigma Value: 0.2, Average Accuracy: 78.79400000000001%\n",
      "Done with Plot fc1\n",
      "Sigma Value: 0.0, Average Accuracy: 85.88%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.32000000000001%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.303%\n",
      "Sigma Value: 0.06, Average Accuracy: 82.10400000000001%\n",
      "Sigma Value: 0.08, Average Accuracy: 80.001%\n",
      "Sigma Value: 0.1, Average Accuracy: 78.213%\n",
      "Sigma Value: 0.12, Average Accuracy: 75.07300000000001%\n",
      "Sigma Value: 0.14, Average Accuracy: 71.086%\n",
      "Sigma Value: 0.16, Average Accuracy: 68.47%\n",
      "Sigma Value: 0.18, Average Accuracy: 64.982%\n",
      "Sigma Value: 0.2, Average Accuracy: 61.64500000000002%\n",
      "Done with Plot fc2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Gaussian Noise\n",
    "sigma_vector = np.linspace(0, 0.2, 11)\n",
    "\n",
    "gaussian_noise_plots_brevitas(perturbations, layer_names, sigma_vector, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
