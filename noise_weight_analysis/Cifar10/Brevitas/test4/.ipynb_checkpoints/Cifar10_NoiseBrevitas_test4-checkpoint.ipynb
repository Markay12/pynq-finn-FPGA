{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8d2528",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'noise_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_385732/729299989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/ashin/source/repos/Cifar10_Pytorch_NoiseAnalysis/Cifar10_Pytorch_NoiseAnalysis/pynq-finn-FPGA/noise_weight_analysis/utils/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnoise_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_clust_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_mask_to_model_brevitas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_noise_plots_brevitas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_digital_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_digital_noise_to_model_brevitas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mber_noise_plot_brevitas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_gaussian_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_gaussian_noise_to_model_brevitas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_noise_plots_brevitas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'noise_functions'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "from unicodedata import decimal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# add imports for randomness\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "# Brevitas imports\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.quant import Int32Bias\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For adaptive learning rate import\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "## Imports from utils file for my defined noise functions\n",
    "import sys\n",
    "#sys.path.append('C:/Users/ashin/source/repos/Cifar10_Pytorch_NoiseAnalysis/Cifar10_Pytorch_NoiseAnalysis/pynq-finn-FPGA/noise_weight_analysis/utils/')\n",
    "sys.path.append('/opt/finn/pynq-finn-FPGA/noise_weight_analysis/utils')\n",
    "\n",
    "from noise_functions import random_clust_mask, add_mask_to_model_brevitas, mask_noise_plots_brevitas, add_digital_noise, add_digital_noise_to_model_brevitas, ber_noise_plot_brevitas, add_gaussian_noise, add_gaussian_noise_to_model_brevitas, gaussian_noise_plots_brevitas, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29bdbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf08b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([128, 3, 32, 32])\n",
      "50000\n",
      "Samples in each set: train = 50000, test = 391\n",
      "Shape of one input sample: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define data augmentation transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "# Use the validation transform for the validation dataset\n",
    "val_set =torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=val_transform)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "a = next(iter(train_loader))\n",
    "print(a[0].size())\n",
    "print(len(train_set))\n",
    "\n",
    "print(\"Samples in each set: train = %d, test = %s\" % (len(train_set), len(train_loader))) \n",
    "print(\"Shape of one input sample: \" +  str(train_set[0][0].shape))\n",
    "\n",
    "## Data Loader\n",
    "#\n",
    "# Using PyTorch dataloader we can create a convenient iterator over the dataset that returns batches of data, rather than requiring manual batch creation.\n",
    "\n",
    "# set batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Create a DataLoader for a training dataset with a batch size of 1000\n",
    "train_quantized_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_quantized_loader = DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d91be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Shape:\n",
      "-------------------------\n",
      "Input shape for 1 batch: torch.Size([128, 3, 32, 32])\n",
      "Label shape for 1 batch: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "print(\"\\nDataset Shape:\\n-------------------------\")\n",
    "for x, y in train_loader:\n",
    "    print(\"Input shape for 1 batch: \" + str(x.shape))\n",
    "    print(\"Label shape for 1 batch: \" + str(y.shape))\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a33c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer1 = qnn.QuantConv2d(3, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer2 = qnn.QuantConv2d(32, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer3 = qnn.QuantConv2d(32, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer4 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer5 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu5 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(64 * 8 * 8, 512, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu6 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc2 = qnn.QuantLinear(512, 10, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant_inp(x)\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.relu2(self.layer2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu3(self.layer3(x))\n",
    "        x = self.relu4(self.layer4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu5(self.layer5(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu6(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bec354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import testing\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Initialize the model, optimizer, and criterion\n",
    "model = CIFAR10CNN().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 80\n",
    "best_test_accuracy = 0\n",
    "patience = 8\n",
    "no_improvement_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a8babc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/391], Loss: 1.7581\n",
      "Epoch [1/80], Step [200/391], Loss: 1.7782\n",
      "Epoch [1/80], Step [300/391], Loss: 1.5384\n",
      "Epoch [1/80], Test Accuracy: 47.48%, Precision: 0.50, Recall: 0.47, F1 score: 0.46\n",
      "Epoch [2/80], Step [100/391], Loss: 1.3843\n",
      "Epoch [2/80], Step [200/391], Loss: 1.4895\n",
      "Epoch [2/80], Step [300/391], Loss: 1.2748\n",
      "Epoch [2/80], Test Accuracy: 54.13%, Precision: 0.56, Recall: 0.54, F1 score: 0.54\n",
      "Epoch [3/80], Step [100/391], Loss: 1.1483\n",
      "Epoch [3/80], Step [200/391], Loss: 1.0081\n",
      "Epoch [3/80], Step [300/391], Loss: 1.1365\n",
      "Epoch [3/80], Test Accuracy: 62.90%, Precision: 0.63, Recall: 0.63, F1 score: 0.62\n",
      "Epoch [4/80], Step [100/391], Loss: 0.9155\n",
      "Epoch [4/80], Step [200/391], Loss: 1.0246\n",
      "Epoch [4/80], Step [300/391], Loss: 0.9774\n",
      "Epoch [4/80], Test Accuracy: 68.16%, Precision: 0.69, Recall: 0.68, F1 score: 0.68\n",
      "Epoch [5/80], Step [100/391], Loss: 0.8218\n",
      "Epoch [5/80], Step [200/391], Loss: 1.0314\n",
      "Epoch [5/80], Step [300/391], Loss: 0.6680\n",
      "Epoch [5/80], Test Accuracy: 71.94%, Precision: 0.72, Recall: 0.72, F1 score: 0.72\n",
      "Epoch [6/80], Step [100/391], Loss: 0.8016\n",
      "Epoch [6/80], Step [200/391], Loss: 0.7677\n",
      "Epoch [6/80], Step [300/391], Loss: 0.7405\n",
      "Epoch [6/80], Test Accuracy: 72.89%, Precision: 0.74, Recall: 0.73, F1 score: 0.73\n",
      "Epoch [7/80], Step [100/391], Loss: 0.9120\n",
      "Epoch [7/80], Step [200/391], Loss: 0.8253\n",
      "Epoch [7/80], Step [300/391], Loss: 0.7368\n",
      "Epoch [7/80], Test Accuracy: 75.04%, Precision: 0.75, Recall: 0.75, F1 score: 0.75\n",
      "Epoch [8/80], Step [100/391], Loss: 0.8806\n",
      "Epoch [8/80], Step [200/391], Loss: 0.5637\n",
      "Epoch [8/80], Step [300/391], Loss: 0.7883\n",
      "Epoch [8/80], Test Accuracy: 77.41%, Precision: 0.77, Recall: 0.77, F1 score: 0.77\n",
      "Epoch [9/80], Step [100/391], Loss: 0.5704\n",
      "Epoch [9/80], Step [200/391], Loss: 0.7288\n",
      "Epoch [9/80], Step [300/391], Loss: 0.7240\n",
      "Epoch [9/80], Test Accuracy: 76.69%, Precision: 0.77, Recall: 0.77, F1 score: 0.77\n",
      "Epoch [10/80], Step [100/391], Loss: 0.6490\n",
      "Epoch [10/80], Step [200/391], Loss: 0.6672\n",
      "Epoch [10/80], Step [300/391], Loss: 0.5111\n",
      "Epoch [10/80], Test Accuracy: 77.79%, Precision: 0.78, Recall: 0.78, F1 score: 0.77\n",
      "Epoch [11/80], Step [100/391], Loss: 0.7193\n",
      "Epoch [11/80], Step [200/391], Loss: 0.6091\n",
      "Epoch [11/80], Step [300/391], Loss: 0.6770\n",
      "Epoch [11/80], Test Accuracy: 78.18%, Precision: 0.78, Recall: 0.78, F1 score: 0.78\n",
      "Epoch [12/80], Step [100/391], Loss: 0.4757\n",
      "Epoch [12/80], Step [200/391], Loss: 0.5535\n",
      "Epoch [12/80], Step [300/391], Loss: 0.5280\n",
      "Epoch [12/80], Test Accuracy: 78.66%, Precision: 0.79, Recall: 0.79, F1 score: 0.79\n",
      "Epoch [13/80], Step [100/391], Loss: 0.5775\n",
      "Epoch [13/80], Step [200/391], Loss: 0.6216\n",
      "Epoch [13/80], Step [300/391], Loss: 0.6164\n",
      "Epoch [13/80], Test Accuracy: 79.93%, Precision: 0.80, Recall: 0.80, F1 score: 0.80\n",
      "Epoch [14/80], Step [100/391], Loss: 0.6831\n",
      "Epoch [14/80], Step [200/391], Loss: 0.4670\n",
      "Epoch [14/80], Step [300/391], Loss: 0.6208\n",
      "Epoch [14/80], Test Accuracy: 78.68%, Precision: 0.80, Recall: 0.79, F1 score: 0.79\n",
      "Epoch [15/80], Step [100/391], Loss: 0.5503\n",
      "Epoch [15/80], Step [200/391], Loss: 0.6811\n",
      "Epoch [15/80], Step [300/391], Loss: 0.6160\n",
      "Epoch [15/80], Test Accuracy: 80.47%, Precision: 0.81, Recall: 0.80, F1 score: 0.81\n",
      "Epoch [16/80], Step [100/391], Loss: 0.4160\n",
      "Epoch [16/80], Step [200/391], Loss: 0.4325\n",
      "Epoch [16/80], Step [300/391], Loss: 0.5355\n",
      "Epoch [16/80], Test Accuracy: 80.83%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [17/80], Step [100/391], Loss: 0.6051\n",
      "Epoch [17/80], Step [200/391], Loss: 0.6533\n",
      "Epoch [17/80], Step [300/391], Loss: 0.6307\n",
      "Epoch [17/80], Test Accuracy: 80.44%, Precision: 0.81, Recall: 0.80, F1 score: 0.80\n",
      "Epoch [18/80], Step [100/391], Loss: 0.3363\n",
      "Epoch [18/80], Step [200/391], Loss: 0.5399\n",
      "Epoch [18/80], Step [300/391], Loss: 0.3899\n",
      "Epoch [18/80], Test Accuracy: 80.67%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [19/80], Step [100/391], Loss: 0.5653\n",
      "Epoch [19/80], Step [200/391], Loss: 0.5613\n",
      "Epoch [19/80], Step [300/391], Loss: 0.4430\n",
      "Epoch [19/80], Test Accuracy: 82.07%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [20/80], Step [100/391], Loss: 0.6895\n",
      "Epoch [20/80], Step [200/391], Loss: 0.4871\n",
      "Epoch [20/80], Step [300/391], Loss: 0.5984\n",
      "Epoch [20/80], Test Accuracy: 82.19%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [21/80], Step [100/391], Loss: 0.5234\n",
      "Epoch [21/80], Step [200/391], Loss: 0.3165\n",
      "Epoch [21/80], Step [300/391], Loss: 0.3694\n",
      "Epoch [21/80], Test Accuracy: 81.84%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [22/80], Step [100/391], Loss: 0.4516\n",
      "Epoch [22/80], Step [200/391], Loss: 0.4931\n",
      "Epoch [22/80], Step [300/391], Loss: 0.5905\n",
      "Epoch [22/80], Test Accuracy: 82.05%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [23/80], Step [100/391], Loss: 0.5298\n",
      "Epoch [23/80], Step [200/391], Loss: 0.4648\n",
      "Epoch [23/80], Step [300/391], Loss: 0.3971\n",
      "Epoch [23/80], Test Accuracy: 81.58%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [24/80], Step [100/391], Loss: 0.5040\n",
      "Epoch [24/80], Step [200/391], Loss: 0.6251\n",
      "Epoch [24/80], Step [300/391], Loss: 0.4162\n",
      "Epoch [24/80], Test Accuracy: 82.70%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [25/80], Step [100/391], Loss: 0.3257\n",
      "Epoch [25/80], Step [200/391], Loss: 0.5446\n",
      "Epoch [25/80], Step [300/391], Loss: 0.4239\n",
      "Epoch [25/80], Test Accuracy: 82.68%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [26/80], Step [100/391], Loss: 0.3806\n",
      "Epoch [26/80], Step [200/391], Loss: 0.3493\n",
      "Epoch [26/80], Step [300/391], Loss: 0.3351\n",
      "Epoch [26/80], Test Accuracy: 81.99%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [27/80], Step [100/391], Loss: 0.4180\n",
      "Epoch [27/80], Step [200/391], Loss: 0.3398\n",
      "Epoch [27/80], Step [300/391], Loss: 0.5127\n",
      "Epoch [27/80], Test Accuracy: 82.03%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [28/80], Step [100/391], Loss: 0.4848\n",
      "Epoch [28/80], Step [200/391], Loss: 0.4115\n",
      "Epoch [28/80], Step [300/391], Loss: 0.4190\n",
      "Epoch [28/80], Test Accuracy: 83.08%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [29/80], Step [100/391], Loss: 0.4946\n",
      "Epoch [29/80], Step [200/391], Loss: 0.3391\n",
      "Epoch [29/80], Step [300/391], Loss: 0.4464\n",
      "Epoch [29/80], Test Accuracy: 82.67%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [30/80], Step [100/391], Loss: 0.3956\n",
      "Epoch [30/80], Step [200/391], Loss: 0.3198\n",
      "Epoch [30/80], Step [300/391], Loss: 0.3413\n",
      "Epoch [30/80], Test Accuracy: 82.78%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [31/80], Step [100/391], Loss: 0.3399\n",
      "Epoch [31/80], Step [200/391], Loss: 0.4714\n",
      "Epoch [31/80], Step [300/391], Loss: 0.3434\n",
      "Epoch [31/80], Test Accuracy: 82.67%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [32/80], Step [100/391], Loss: 0.4009\n",
      "Epoch [32/80], Step [200/391], Loss: 0.4383\n",
      "Epoch [32/80], Step [300/391], Loss: 0.3666\n",
      "Epoch [32/80], Test Accuracy: 82.43%, Precision: 0.83, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [33/80], Step [100/391], Loss: 0.3740\n",
      "Epoch [33/80], Step [200/391], Loss: 0.2418\n",
      "Epoch [33/80], Step [300/391], Loss: 0.4185\n",
      "Epoch [33/80], Test Accuracy: 82.78%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [34/80], Step [100/391], Loss: 0.4220\n",
      "Epoch [34/80], Step [200/391], Loss: 0.3278\n",
      "Epoch [34/80], Step [300/391], Loss: 0.3743\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [34/80], Test Accuracy: 82.60%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [35/80], Step [100/391], Loss: 0.3853\n",
      "Epoch [35/80], Step [200/391], Loss: 0.4061\n",
      "Epoch [35/80], Step [300/391], Loss: 0.3725\n",
      "Epoch [35/80], Test Accuracy: 84.78%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [36/80], Step [100/391], Loss: 0.1976\n",
      "Epoch [36/80], Step [200/391], Loss: 0.2598\n",
      "Epoch [36/80], Step [300/391], Loss: 0.2799\n",
      "Epoch [36/80], Test Accuracy: 85.00%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [37/80], Step [100/391], Loss: 0.3504\n",
      "Epoch [37/80], Step [200/391], Loss: 0.3695\n",
      "Epoch [37/80], Step [300/391], Loss: 0.2786\n",
      "Epoch [37/80], Test Accuracy: 85.08%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [38/80], Step [100/391], Loss: 0.4122\n",
      "Epoch [38/80], Step [200/391], Loss: 0.3063\n",
      "Epoch [38/80], Step [300/391], Loss: 0.2919\n",
      "Epoch [38/80], Test Accuracy: 84.95%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/80], Step [100/391], Loss: 0.2842\n",
      "Epoch [39/80], Step [200/391], Loss: 0.2820\n",
      "Epoch [39/80], Step [300/391], Loss: 0.2912\n",
      "Epoch [39/80], Test Accuracy: 85.10%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [40/80], Step [100/391], Loss: 0.3431\n",
      "Epoch [40/80], Step [200/391], Loss: 0.4236\n",
      "Epoch [40/80], Step [300/391], Loss: 0.3841\n",
      "Epoch [40/80], Test Accuracy: 85.27%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [41/80], Step [100/391], Loss: 0.2618\n",
      "Epoch [41/80], Step [200/391], Loss: 0.4188\n",
      "Epoch [41/80], Step [300/391], Loss: 0.3041\n",
      "Epoch [41/80], Test Accuracy: 85.17%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [42/80], Step [100/391], Loss: 0.2462\n",
      "Epoch [42/80], Step [200/391], Loss: 0.3617\n",
      "Epoch [42/80], Step [300/391], Loss: 0.2576\n",
      "Epoch [42/80], Test Accuracy: 85.42%, Precision: 0.86, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [43/80], Step [100/391], Loss: 0.2355\n",
      "Epoch [43/80], Step [200/391], Loss: 0.3305\n",
      "Epoch [43/80], Step [300/391], Loss: 0.2448\n",
      "Epoch [43/80], Test Accuracy: 85.16%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [44/80], Step [100/391], Loss: 0.2337\n",
      "Epoch [44/80], Step [200/391], Loss: 0.2942\n",
      "Epoch [44/80], Step [300/391], Loss: 0.2459\n",
      "Epoch [44/80], Test Accuracy: 85.37%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [45/80], Step [100/391], Loss: 0.2364\n",
      "Epoch [45/80], Step [200/391], Loss: 0.2288\n",
      "Epoch [45/80], Step [300/391], Loss: 0.2341\n",
      "Epoch [45/80], Test Accuracy: 85.21%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [46/80], Step [100/391], Loss: 0.2942\n",
      "Epoch [46/80], Step [200/391], Loss: 0.2354\n",
      "Epoch [46/80], Step [300/391], Loss: 0.1502\n",
      "Epoch [46/80], Test Accuracy: 85.32%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [47/80], Step [100/391], Loss: 0.2755\n",
      "Epoch [47/80], Step [200/391], Loss: 0.2466\n",
      "Epoch [47/80], Step [300/391], Loss: 0.2457\n",
      "Epoch [47/80], Test Accuracy: 85.40%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [48/80], Step [100/391], Loss: 0.2227\n",
      "Epoch [48/80], Step [200/391], Loss: 0.2838\n",
      "Epoch [48/80], Step [300/391], Loss: 0.2887\n",
      "Epoch [48/80], Test Accuracy: 85.36%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [49/80], Step [100/391], Loss: 0.2738\n",
      "Epoch [49/80], Step [200/391], Loss: 0.1826\n",
      "Epoch [49/80], Step [300/391], Loss: 0.3098\n",
      "Epoch [49/80], Test Accuracy: 85.24%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [50/80], Step [100/391], Loss: 0.3175\n",
      "Epoch [50/80], Step [200/391], Loss: 0.2611\n",
      "Epoch [50/80], Step [300/391], Loss: 0.2996\n",
      "Epoch [50/80], Test Accuracy: 85.45%, Precision: 0.86, Recall: 0.85, F1 score: 0.86\n",
      "Epoch [51/80], Step [100/391], Loss: 0.2363\n",
      "Epoch [51/80], Step [200/391], Loss: 0.2093\n",
      "Epoch [51/80], Step [300/391], Loss: 0.3499\n",
      "Epoch [51/80], Test Accuracy: 85.21%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [52/80], Step [100/391], Loss: 0.2732\n",
      "Epoch [52/80], Step [200/391], Loss: 0.2648\n",
      "Epoch [52/80], Step [300/391], Loss: 0.3473\n",
      "Epoch [52/80], Test Accuracy: 85.18%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [53/80], Step [100/391], Loss: 0.1872\n",
      "Epoch [53/80], Step [200/391], Loss: 0.2043\n",
      "Epoch [53/80], Step [300/391], Loss: 0.2742\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [53/80], Test Accuracy: 85.65%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [54/80], Step [100/391], Loss: 0.2709\n",
      "Epoch [54/80], Step [200/391], Loss: 0.2192\n",
      "Epoch [54/80], Step [300/391], Loss: 0.1595\n",
      "Epoch [54/80], Test Accuracy: 85.62%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [55/80], Step [100/391], Loss: 0.3372\n",
      "Epoch [55/80], Step [200/391], Loss: 0.2548\n",
      "Epoch [55/80], Step [300/391], Loss: 0.2574\n",
      "Epoch [55/80], Test Accuracy: 85.53%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [56/80], Step [100/391], Loss: 0.2412\n",
      "Epoch [56/80], Step [200/391], Loss: 0.2721\n",
      "Epoch [56/80], Step [300/391], Loss: 0.1675\n",
      "Epoch [56/80], Test Accuracy: 85.88%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [57/80], Step [100/391], Loss: 0.3224\n",
      "Epoch [57/80], Step [200/391], Loss: 0.1434\n",
      "Epoch [57/80], Step [300/391], Loss: 0.3064\n",
      "Epoch [57/80], Test Accuracy: 85.81%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [58/80], Step [100/391], Loss: 0.1886\n",
      "Epoch [58/80], Step [200/391], Loss: 0.2931\n",
      "Epoch [58/80], Step [300/391], Loss: 0.1887\n",
      "Epoch [58/80], Test Accuracy: 85.60%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [59/80], Step [100/391], Loss: 0.2622\n",
      "Epoch [59/80], Step [200/391], Loss: 0.1352\n",
      "Epoch [59/80], Step [300/391], Loss: 0.1795\n",
      "Epoch [59/80], Test Accuracy: 85.80%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [60/80], Step [100/391], Loss: 0.2411\n",
      "Epoch [60/80], Step [200/391], Loss: 0.3285\n",
      "Epoch [60/80], Step [300/391], Loss: 0.4192\n",
      "Epoch [60/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [61/80], Step [100/391], Loss: 0.2320\n",
      "Epoch [61/80], Step [200/391], Loss: 0.2958\n",
      "Epoch [61/80], Step [300/391], Loss: 0.2420\n",
      "Epoch [61/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [62/80], Step [100/391], Loss: 0.3244\n",
      "Epoch [62/80], Step [200/391], Loss: 0.2040\n",
      "Epoch [62/80], Step [300/391], Loss: 0.1512\n",
      "Epoch [62/80], Test Accuracy: 85.73%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [63/80], Step [100/391], Loss: 0.2197\n",
      "Epoch [63/80], Step [200/391], Loss: 0.2291\n",
      "Epoch [63/80], Step [300/391], Loss: 0.2767\n",
      "Epoch [63/80], Test Accuracy: 85.95%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [64/80], Step [100/391], Loss: 0.2288\n",
      "Epoch [64/80], Step [200/391], Loss: 0.2318\n",
      "Epoch [64/80], Step [300/391], Loss: 0.1853\n",
      "Epoch [64/80], Test Accuracy: 85.94%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [65/80], Step [100/391], Loss: 0.2293\n",
      "Epoch [65/80], Step [200/391], Loss: 0.2891\n",
      "Epoch [65/80], Step [300/391], Loss: 0.1683\n",
      "Epoch [65/80], Test Accuracy: 85.65%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [66/80], Step [100/391], Loss: 0.1923\n",
      "Epoch [66/80], Step [200/391], Loss: 0.1138\n",
      "Epoch [66/80], Step [300/391], Loss: 0.2055\n",
      "Epoch [66/80], Test Accuracy: 85.64%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [67/80], Step [100/391], Loss: 0.1742\n",
      "Epoch [67/80], Step [200/391], Loss: 0.3278\n",
      "Epoch [67/80], Step [300/391], Loss: 0.2256\n",
      "Epoch [67/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [68/80], Step [100/391], Loss: 0.1497\n",
      "Epoch [68/80], Step [200/391], Loss: 0.1958\n",
      "Epoch [68/80], Step [300/391], Loss: 0.2030\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch [68/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [69/80], Step [100/391], Loss: 0.2898\n",
      "Epoch [69/80], Step [200/391], Loss: 0.2270\n",
      "Epoch [69/80], Step [300/391], Loss: 0.2735\n",
      "Epoch [69/80], Test Accuracy: 85.89%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [70/80], Step [100/391], Loss: 0.2381\n",
      "Epoch [70/80], Step [200/391], Loss: 0.2529\n",
      "Epoch [70/80], Step [300/391], Loss: 0.1969\n",
      "Epoch [70/80], Test Accuracy: 85.99%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [71/80], Step [100/391], Loss: 0.1510\n",
      "Epoch [71/80], Step [200/391], Loss: 0.2830\n",
      "Epoch [71/80], Step [300/391], Loss: 0.3289\n",
      "Epoch [71/80], Test Accuracy: 85.90%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [72/80], Step [100/391], Loss: 0.2035\n",
      "Epoch [72/80], Step [200/391], Loss: 0.2620\n",
      "Epoch [72/80], Step [300/391], Loss: 0.2721\n",
      "Epoch [72/80], Test Accuracy: 85.91%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [73/80], Step [100/391], Loss: 0.2259\n",
      "Epoch [73/80], Step [200/391], Loss: 0.3350\n",
      "Epoch [73/80], Step [300/391], Loss: 0.3486\n",
      "Epoch [73/80], Test Accuracy: 85.99%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [74/80], Step [100/391], Loss: 0.2075\n",
      "Epoch [74/80], Step [200/391], Loss: 0.2492\n",
      "Epoch [74/80], Step [300/391], Loss: 0.2433\n",
      "Epoch [74/80], Test Accuracy: 85.71%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [75/80], Step [100/391], Loss: 0.2900\n",
      "Epoch [75/80], Step [200/391], Loss: 0.2705\n",
      "Epoch [75/80], Step [300/391], Loss: 0.1964\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch [75/80], Test Accuracy: 85.90%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [76/80], Step [100/391], Loss: 0.1782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/80], Step [200/391], Loss: 0.2292\n",
      "Epoch [76/80], Step [300/391], Loss: 0.2834\n",
      "Epoch [76/80], Test Accuracy: 85.79%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [77/80], Step [100/391], Loss: 0.2393\n",
      "Epoch [77/80], Step [200/391], Loss: 0.2057\n",
      "Epoch [77/80], Step [300/391], Loss: 0.3019\n",
      "Epoch [77/80], Test Accuracy: 85.83%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [78/80], Step [100/391], Loss: 0.2675\n",
      "Epoch [78/80], Step [200/391], Loss: 0.2053\n",
      "Epoch [78/80], Step [300/391], Loss: 0.2695\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # training phase\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "    \n",
    "    # Initialize the validation loss\n",
    "    val_loss = 0\n",
    "    \n",
    "    # testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()  # accumulate the validation loss\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "        val_loss /= len(val_loader)  # calculate the average validation loss\n",
    "    \n",
    "        # Update the learning rate using the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        test_accuracy = 100 * correct / total\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), 'test4_model.pth')\n",
    "            no_improvement_counter = 0\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print('Epoch [{}/{}], Test Accuracy: {:.2f}%, Precision: {:.2f}, Recall: {:.2f}, F1 score: {:.2f}'.format(epoch+1, num_epochs, test_accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055fe5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final best test accuracy is: 85.99%\n"
     ]
    }
   ],
   "source": [
    "# Print the best test accuracy\n",
    "print(\"The final best test accuracy is: {:.2f}%\".format(best_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966f95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b94d7e",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d3d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_accuracy > best_test_accuracy:\n",
    "    best_test_accuracy = test_accuracy\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_test_accuracy': best_test_accuracy,\n",
    "        'epoch': epoch\n",
    "        }, 'best_model.pth')\n",
    "    no_improvement_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3573bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of your neural network model\n",
    "model = CIFAR10CNN().to(device)\n",
    "\n",
    "# Load the saved state dictionary from file\n",
    "state_dict = torch.load('test4_model.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359fd9d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER Value: 1e-06\tAverage Accuracy: 86.0\n",
      "BER Value: 1e-05\tAverage Accuracy: 86.0\n",
      "BER Value: 0.0001\tAverage Accuracy: 86.0\n",
      "BER Value: 0.001\tAverage Accuracy: 84.833\n",
      "BER Value: 0.01\tAverage Accuracy: 79.701\n",
      "BER Value: 0.1\tAverage Accuracy: 47.328\n",
      "Done with Plot layer1\n",
      "BER Value: 1e-06\tAverage Accuracy: 86.0\n",
      "BER Value: 1e-05\tAverage Accuracy: 86.0\n",
      "BER Value: 0.0001\tAverage Accuracy: 84.94099999999999\n",
      "BER Value: 0.001\tAverage Accuracy: 83.392\n",
      "BER Value: 0.01\tAverage Accuracy: 79.116\n",
      "BER Value: 0.1\tAverage Accuracy: 32.993\n",
      "Done with Plot layer2\n",
      "BER Value: 1e-06\tAverage Accuracy: 86.0\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.825\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.44900000000001\n",
      "BER Value: 0.001\tAverage Accuracy: 84.834\n",
      "BER Value: 0.01\tAverage Accuracy: 82.779\n",
      "BER Value: 0.1\tAverage Accuracy: 55.690999999999995\n",
      "Done with Plot layer3\n",
      "BER Value: 1e-06\tAverage Accuracy: 86.0\n",
      "BER Value: 1e-05\tAverage Accuracy: 86.0\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.29100000000001\n",
      "BER Value: 0.001\tAverage Accuracy: 85.028\n",
      "BER Value: 0.01\tAverage Accuracy: 83.31300000000002\n",
      "BER Value: 0.1\tAverage Accuracy: 55.577\n",
      "Done with Plot layer4\n",
      "BER Value: 1e-06\tAverage Accuracy: 86.0\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.806\n",
      "BER Value: 0.0001\tAverage Accuracy: 84.937\n",
      "BER Value: 0.001\tAverage Accuracy: 83.71999999999998\n",
      "BER Value: 0.01\tAverage Accuracy: 81.103\n",
      "BER Value: 0.1\tAverage Accuracy: 57.137\n",
      "Done with Plot layer5\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.78999999999999\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.583\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.53599999999999\n",
      "BER Value: 0.001\tAverage Accuracy: 85.50899999999999\n",
      "BER Value: 0.01\tAverage Accuracy: 85.023\n",
      "BER Value: 0.1\tAverage Accuracy: 79.525\n",
      "Done with Plot fc1\n",
      "BER Value: 1e-06\tAverage Accuracy: 86.0\n",
      "BER Value: 1e-05\tAverage Accuracy: 86.0\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.82399999999998\n",
      "BER Value: 0.001\tAverage Accuracy: 84.85300000000001\n",
      "BER Value: 0.01\tAverage Accuracy: 80.92299999999999\n",
      "BER Value: 0.1\tAverage Accuracy: 61.690999999999995\n",
      "Done with Plot fc2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Digital Noise with BER\n",
    "layer_names = ['layer1', 'layer2', 'layer3', 'layer4', 'layer5', 'fc1', 'fc2']\n",
    "ber_vals = [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1]\n",
    "perturbations = 10\n",
    "\n",
    "ber_noise_plot_brevitas(perturbations, layer_names, ber_vals, model, device, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cabe2b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma Value: 0.0, Average Accuracy: 86.0%\n",
      "Sigma Value: 0.02, Average Accuracy: 84.78100000000002%\n",
      "Sigma Value: 0.04, Average Accuracy: 81.703%\n",
      "Sigma Value: 0.06, Average Accuracy: 74.405%\n",
      "Sigma Value: 0.08, Average Accuracy: 71.20500000000001%\n",
      "Sigma Value: 0.1, Average Accuracy: 65.415%\n",
      "Sigma Value: 0.12, Average Accuracy: 52.909000000000006%\n",
      "Sigma Value: 0.14, Average Accuracy: 45.673%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Gaussian Noise\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sigma_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mgaussian_noise_plots_brevitas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_quantized_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users/ashin/source/repos/Cifar10_Pytorch_NoiseAnalysis/Cifar10_Pytorch_NoiseAnalysis/pynq-finn-FPGA/noise_weight_analysis/utils\\noise_functions.py:577\u001b[0m, in \u001b[0;36mgaussian_noise_plots_brevitas\u001b[1;34m(num_perturbations, layer_names, sigma_vector, model, device, test_quantized_loader)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noisy_model \u001b[38;5;129;01min\u001b[39;00m noisy_models:\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Move the model back to the target device\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     noisy_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 577\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_quantized_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;66;03m# Calculate the average accuracy and print the result\u001b[39;00m\n\u001b[0;32m    579\u001b[0m avg_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(accuracies) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(accuracies)\n",
      "File \u001b[1;32mC:\\Users/ashin/source/repos/Cifar10_Pytorch_NoiseAnalysis/Cifar10_Pytorch_NoiseAnalysis/pynq-finn-FPGA/noise_weight_analysis/utils\\noise_functions.py:627\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m    625\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    626\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 627\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    629\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mCIFAR10CNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_inp(x)\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x))\n\u001b[1;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x))\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\nn\\quant_conv.py:189\u001b[0m, in \u001b[0;36mQuantConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\nn\\quant_layer.py:317\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m    316\u001b[0m quant_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_quant(inp)\n\u001b[1;32m--> 317\u001b[0m quant_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quant_input\u001b[38;5;241m.\u001b[39mbit_width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     output_bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_acc_bit_width(quant_input\u001b[38;5;241m.\u001b[39mbit_width, quant_weight\u001b[38;5;241m.\u001b[39mbit_width)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\nn\\mixin\\parameter.py:55\u001b[0m, in \u001b[0;36mQuantWeightMixin.quant_weight\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquant_weight\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\proxy\\parameter_quant.py:86\u001b[0m, in \u001b[0;36mWeightQuantProxyFromInjector.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quant_enabled:\n\u001b[0;32m     85\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_handler \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_quant\n\u001b[1;32m---> 86\u001b[0m     out, scale, zero_point, bit_width \u001b[38;5;241m=\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QuantTensor(out, scale, zero_point, bit_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_signed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# quantization disabled\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\core\\quant\\int.py:159\u001b[0m, in \u001b[0;36mRescalingIntQuant.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    157\u001b[0m bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsb_clamp_bit_width_impl()\n\u001b[0;32m    158\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_impl(x)\n\u001b[1;32m--> 159\u001b[0m int_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_scaling_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m scale \u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;241m/\u001b[39m int_threshold\n\u001b[0;32m    161\u001b[0m zero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point_impl(x, scale, bit_width)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\core\\scaling\\int_scaling.py:22\u001b[0m, in \u001b[0;36mIntScaling.forward\u001b[1;34m(self, bit_width)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, bit_width: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigned:\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m \u001b[43mmin_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnarrow_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m max_int(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigned, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnarrow_range, bit_width)\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\brevitas\\function\\ops.py:186\u001b[0m, in \u001b[0;36mmin_int\u001b[1;34m(signed, narrow_range, bit_width)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Compute the minimum integer representable by a given number of bits.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    tensor(0)\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m signed \u001b[38;5;129;01mand\u001b[39;00m narrow_range:\n\u001b[1;32m--> 186\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m signed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m narrow_range:\n\u001b[0;32m    188\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (bit_width \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\PythonEnvironments\\TorchEnv\\Lib\\site-packages\\torch\\_tensor.py:878\u001b[0m, in \u001b[0;36mTensor.__rpow__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rpow__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m    877\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mresult_type(other, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Gaussian Noise\n",
    "sigma_vector = np.linspace(0, 0.2, 11)\n",
    "\n",
    "gaussian_noise_plots_brevitas(perturbations, layer_names, sigma_vector, model, device, test_quantized_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
