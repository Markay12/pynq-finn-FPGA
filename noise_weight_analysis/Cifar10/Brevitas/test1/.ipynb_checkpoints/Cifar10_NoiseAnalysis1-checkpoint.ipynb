{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8d2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "from unicodedata import decimal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# add imports for randomness\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "# Brevitas imports\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.quant import Int32Bias\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For adaptive learning rate import\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "## Imports from utils file for my defined noise functions\n",
    "import sys\n",
    "sys.path.append('C:/Users/ashin/source/repos/Cifar10_Pytorch_NoiseAnalysis/Cifar10_Pytorch_NoiseAnalysis/pynq-finn-FPGA/noise_weight_analysis/utils/')\n",
    "\n",
    "from noise_functions import random_clust_mask, add_mask_to_model_brevitas, mask_noise_plots_brevitas, add_digital_noise, add_digital_noise_to_model_brevitas, ber_noise_plot_brevitas, add_gaussian_noise_to_model_brevitas, gaussian_noise_plots_brevitas, test\n",
    "from noise_functions import add_gaussian_noise_independent, add_gaussian_noise_proportional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29bdbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf08b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([128, 3, 32, 32])\n",
      "50000\n",
      "Samples in each set: train = 50000, test = 391\n",
      "Shape of one input sample: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define data augmentation transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "# Use the validation transform for the validation dataset\n",
    "val_set =torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=val_transform)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "a = next(iter(train_loader))\n",
    "print(a[0].size())\n",
    "print(len(train_set))\n",
    "\n",
    "print(\"Samples in each set: train = %d, test = %s\" % (len(train_set), len(train_loader))) \n",
    "print(\"Shape of one input sample: \" +  str(train_set[0][0].shape))\n",
    "\n",
    "## Data Loader\n",
    "#\n",
    "# Using PyTorch dataloader we can create a convenient iterator over the dataset that returns batches of data, rather than requiring manual batch creation.\n",
    "\n",
    "# set batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Create a DataLoader for a training dataset with a batch size of 1000\n",
    "train_quantized_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_quantized_loader = DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d91be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Shape:\n",
      "-------------------------\n",
      "Input shape for 1 batch: torch.Size([128, 3, 32, 32])\n",
      "Label shape for 1 batch: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "print(\"\\nDataset Shape:\\n-------------------------\")\n",
    "for x, y in train_loader:\n",
    "    print(\"Input shape for 1 batch: \" + str(x.shape))\n",
    "    print(\"Label shape for 1 batch: \" + str(y.shape))\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a33c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer1 = qnn.QuantConv2d(3, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer2 = qnn.QuantConv2d(32, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer3 = qnn.QuantConv2d(32, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer4 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer5 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu5 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(64 * 8 * 8, 512, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu6 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc2 = qnn.QuantLinear(512, 10, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant_inp(x)\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.relu2(self.layer2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu3(self.layer3(x))\n",
    "        x = self.relu4(self.layer4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu5(self.layer5(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu6(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bec354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import testing\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Initialize the model, optimizer, and criterion\n",
    "model = CIFAR10CNN().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 80\n",
    "best_test_accuracy = 0\n",
    "patience = 8\n",
    "no_improvement_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8babc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # training phase\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "    \n",
    "    # Initialize the validation loss\n",
    "    val_loss = 0\n",
    "    \n",
    "    # testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()  # accumulate the validation loss\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "        val_loss /= len(val_loader)  # calculate the average validation loss\n",
    "    \n",
    "        # Update the learning rate using the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        test_accuracy = 100 * correct / total\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            no_improvement_counter = 0\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print('Epoch [{}/{}], Test Accuracy: {:.2f}%, Precision: {:.2f}, Recall: {:.2f}, F1 score: {:.2f}'.format(epoch+1, num_epochs, test_accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fe5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best test accuracy\n",
    "print(\"The final best test accuracy is: {:.2f}%\".format(best_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b94d7e",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_accuracy > best_test_accuracy:\n",
    "    best_test_accuracy = test_accuracy\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_test_accuracy': best_test_accuracy,\n",
    "        'epoch': epoch\n",
    "        }, 'best_model.pth')\n",
    "    no_improvement_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3573bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of your neural network model\n",
    "model = CIFAR10CNN().to(device)\n",
    "\n",
    "# Load the saved state dictionary from file\n",
    "state_dict = torch.load('best_model.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479c0caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 3])\n",
      "torch.Size([32, 32, 3, 3])\n",
      "torch.Size([512, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(model.layer1.weight.shape)\n",
    "print(model.layer2.weight.shape)\n",
    "print(model.fc1.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f829b",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "## Mask Code Here for Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "617632fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_clust_mask(weight, P, gamma):\n",
    "\n",
    "    # Generate random NxN matrix with values between 0 and 1\n",
    "    # matrix = np.random.rand(N, N)\n",
    "    \n",
    "        \n",
    "    N = weight.shape[0]\n",
    "    M = weight.shape[1]\n",
    "    \n",
    "    weight_numpy = weight.cpu().numpy()\n",
    "    \n",
    "    if (N  > M):\n",
    "        \n",
    "        matrix = np.random.rand(N, N)\n",
    "        L = N\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        matrix = np.random.rand(M, M)\n",
    "        L = M\n",
    "    \n",
    "            \n",
    "    matrix_tensor = torch.tensor(matrix)\n",
    "    \n",
    "    # Compute 2D FFT\n",
    "    fft_result = np.fft.fft2(matrix)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 1D Frequency Vector with N bins\n",
    "    f = np.fft.fftfreq(L, d=1.0/L)\n",
    "    f_x, f_y = np.meshgrid(f, f)\n",
    "    f_x[0] = 1e-6\n",
    "    f_y[0] = 1e-6\n",
    "\n",
    "    # Create a 2D filter in frequency space that varies inversely with freq over f\n",
    "    # Gamma controls the falloff rate\n",
    "    filter_2D = 1/(np.sqrt(f_x**2 + f_y**2))**gamma\n",
    "\n",
    "    # Mult the 2D elementwise by the filter\n",
    "    filtered_fft = fft_result * filter_2D\n",
    "\n",
    "    # 2D inverse FFT of the filtered result\n",
    "    ifft_result = np.fft.ifft2(filtered_fft)\n",
    "    ifft_result = np.real(ifft_result)\n",
    "\n",
    "    # Set the threshold T equal the the max value in IFFT\n",
    "    T = ifft_result.max()\n",
    "\n",
    "    # Init empty bool mask with same dims as ifft\n",
    "    mask = np.zeros_like(ifft_result, dtype=bool)\n",
    "\n",
    "    decrement_step = 0.01\n",
    "\n",
    "    # Repeat until frac of nonzero values in the mask is greater than or equal to P\n",
    "    while True:\n",
    "        mask = ifft_result > T\n",
    "\n",
    "        current_fraction = np.count_nonzero(mask) / (N * N)\n",
    "\n",
    "        if current_fraction >= P:\n",
    "            break\n",
    "\n",
    "        T -= decrement_step\n",
    "\n",
    "    # Return tensor with the same shape as the input tensor\n",
    "    # mask = np.tile(mask, (weight_shape[0], weight_shape[1], 1, 1))\n",
    "    \n",
    "    if (N > M):\n",
    "        \n",
    "        mask = mask[:,:M]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        mask = mask[:N,:]\n",
    "        \n",
    "    mask_final = np.zeros_like(weight_numpy)\n",
    "    #print(mask_final.shape)\n",
    "    #print(mask.shape)\n",
    "    \n",
    "    mask_tensor = torch.tensor(mask, dtype=torch.bool, device=weight.device)\n",
    "    mask_final = torch.zeros_like(weight)\n",
    "    # Create a temporary tensor that repeats the mask tensor along the 3rd dimension\n",
    "    temp_mask = mask_tensor.unsqueeze(2).repeat(1, 1, weight.shape[2])\n",
    "\n",
    "    # Copy the temporary mask tensor along the 4th dimension (axis=3) of the mask_final tensor\n",
    "    for i in range(mask_final.shape[3]):\n",
    "        mask_final[:, :, :, i] = temp_mask\n",
    "        \n",
    "    mask_logical = torch.zeros(weight.shape, dtype=torch.bool,device=weight.device)\n",
    "    mask_logical = (mask_final==1)\n",
    "    mask_numeric = mask_final\n",
    "    #mask tensor is 2D and float type\n",
    "    #mask numeric/final is 4D and float like\n",
    "    #mask logical is 4D and bool type\n",
    "    return mask_tensor, mask_numeric,mask_logical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_noisy_matrix(independent, dim_matrix, sigma):\n",
    "    \n",
    "    if (independent):\n",
    "        noised_matrix = torch.randn(dim_matrix) * sigma\n",
    "    else:\n",
    "        noised_matrix = torch.randn(dim_matrix) * sigma + 1\n",
    "\n",
    "    return noised_matrix.to(device)\n",
    "\n",
    "\"\"\"\n",
    "This function takes a brevitas layer, a mask generated by the random_clust_mask function,\n",
    "and two parameters P and gamma. It generates a random clustered mask using the random_clust_mask\n",
    "function, and then sets the weights at the locations in the mask to zero, effectively sparsifying\n",
    "the layer's weights.\n",
    "\"\"\"\n",
    "\n",
    "def add_mask_to_model_brevitas(model, layer_names, p, gamma, num_perturbations, sigma, independent_type, print_weights=False):\n",
    "\n",
    "    modified_models = []\n",
    "\n",
    "    for _ in range(num_perturbations):\n",
    "\n",
    "        modified_model = deepcopy(model)\n",
    "\n",
    "        for layer_name in layer_names:\n",
    "\n",
    "            layer = getattr(modified_model, layer_name)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # get weights of the tensors\n",
    "                weight_tensor = layer.weight.clone().detach()\n",
    "                \n",
    "                #print(weight_tensor.shape)\n",
    "                \n",
    "                # generate mask with correct shape\n",
    "                mask_graph,mask_numeric,mask_logical = random_clust_mask(weight_tensor, p, gamma)\n",
    "                \n",
    "\n",
    "                if print_weights:\n",
    "                    print(\"Weights before masking:\")\n",
    "                    print(weight_tensor)\n",
    "                    \n",
    "                #plt.imshow(weight_tensor.cpu().numpy(), cmap='viridis', aspect = 'auto')\n",
    "                #plt.colorbar()\n",
    "                #plt.show()\n",
    "                #plt.clf()\n",
    "\n",
    "                # apply mask to the whole weight tensor\n",
    "                #print(\"mask logical = \",mask_logical)\n",
    "                #print(\"mask numeric = \",mask_numeric)\n",
    "                                \n",
    "                #plt.imshow(mask_graph.cpu().numpy(), cmap='viridis', aspect = 'auto')\n",
    "                #plt.colorbar()\n",
    "                #plt.show()\n",
    "                #plt.clf()\n",
    "                \n",
    "                if (independent_type):\n",
    "                    weight_tensor += mask_numeric * return_noisy_matrix(independent_type, weight_tensor.shape, sigma)\n",
    "                else:\n",
    "                    noisy_mat = return_noisy_matrix(independent_type, weight_tensor.shape, sigma)\n",
    "                    noisy_mat[~mask_logical] =  1\n",
    "                    weight_tensor *= noisy_mat\n",
    "                \n",
    "                if print_weights:\n",
    "                    print(\"Weights after masking:\")\n",
    "                    print(weight_tensor)\n",
    "                    \n",
    "                #plt.imshow(weight_tensor.cpu().numpy(), cmap='viridis', aspect = 'auto')\n",
    "                #plt.colorbar()\n",
    "                #plt.show()\n",
    "                #plt.clf()\n",
    "\n",
    "                # create new weight parameter and assign to layer\n",
    "                noised_weight = torch.nn.Parameter(weight_tensor, requires_grad=False)\n",
    "                \n",
    "                \n",
    "                layer.weight = noised_weight\n",
    "\n",
    "        modified_models.append(modified_model)\n",
    "\n",
    "    return modified_models\n",
    "\n",
    "\"\"\"\n",
    "The function mask_noise_plots_brevitas() is a Python function that is used to visualize the effect of different perturbations on the accuracy of a neural network. \n",
    "It generates several visualizations, including a heatmap and a scatter plot.\n",
    "\n",
    "The function takes in the following parameters:\n",
    "\n",
    "    num_perturbations: an integer indicating the number of perturbations to apply to the neural network\n",
    "    layer_names: a list of strings indicating the names of the layers in the neural network\n",
    "    p_vals: a list of floats indicating the p values to use for the mask perturbation\n",
    "    gamma_vals: a list of floats indicating the gamma values to use for the mask perturbation\n",
    "    model: the neural network model to test\n",
    "    device: the device to use for testing the model\n",
    "\n",
    "The function begins by creating a directory for saving the output plots. \n",
    "It then initializes an empty list called all_test_accs to store the test accuracies for each layer.\n",
    "\n",
    "For each layer in layer_names, the function initializes an empty list called test_accs to store \n",
    "the test accuracies for each mask perturbation. The function then iterates over each p and gamma value \n",
    "and adds noise to the model for the defined layer only. It then tests the accuracy of each noisy model \n",
    "and appends the result to the accuracies list. The function then calculates the average accuracy \n",
    "and prints the result.\n",
    "\n",
    "The test accuracies for the current layer are then stored in the all_test_accs list.\n",
    "\n",
    "The function then creates a heatmap for each layer and saves it to disk.\n",
    "It also computes the average test accuracy across all layers for each p and gamma value and creates a\n",
    "heatmap for the average test accuracy. Both the individual and average heatmaps have a color bar \n",
    "indicating the test accuracy.\n",
    "\n",
    "Finally, the function creates a scatter plot showing the test accuracies for each layer at each p \n",
    "and gamma value. The plot has layer_names on the x-axis, p_vals on the y-axis, and gamma_vals on the z-axis. \n",
    "The points in the plot are colored based on the corresponding test accuracy, with a color bar indicating \n",
    "the mapping between color and test accuracy.\n",
    "\"\"\"\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    # testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def mask_noise_plots_brevitas(num_perturbations, layer_names, p_values, gamma_values, model, device, sigma_values, independent_type):\n",
    "    # Initialize a 3D array to store average accuracies for each combination of p, gamma, and sigma values\n",
    "    avg_accuracies = np.zeros((len(p_values), len(gamma_values), len(sigma_values)))\n",
    "\n",
    "    for p in range(len(p_values)):\n",
    "        for g in range(len(gamma_values)):\n",
    "            for s in range(len(sigma_values)):\n",
    "                noisy_models = add_mask_to_model_brevitas(model, layer_names, p_values[p], gamma_values[g], num_perturbations, sigma_values[s], independent_type)\n",
    "\n",
    "                accuracies = []\n",
    "\n",
    "                for noisy_model in noisy_models:\n",
    "                    noisy_model.to(device)\n",
    "                    accuracies.append(test(noisy_model, test_quantized_loader, device))\n",
    "\n",
    "                avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "                avg_accuracies[p, g, s] = avg_accuracy\n",
    "                print(\"P Value: {}\\t Gamma Value: {}\\t Sigma Value: {}\\t Average Accuracy: {}%\".format(p_values[p], gamma_values[g], sigma_values[s], avg_accuracy))\n",
    "\n",
    "    # Create a heatmap for each sigma value\n",
    "    for s in range(len(sigma_values)):\n",
    "        plt.figure()\n",
    "        plt.imshow(avg_accuracies[:, :, s], cmap='viridis', aspect='auto', origin='lower', extent=[gamma_values[0], gamma_values[-1], p_values[0], p_values[-1]])\n",
    "        plt.colorbar(label='Average Accuracy')\n",
    "        plt.xlabel('Gamma Value')\n",
    "        plt.ylabel('P Value')\n",
    "        plt.title(f'Average Accuracy Heatmap (Sigma = {sigma_values[s]})')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b21c6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 3])\n",
      "P Value: 0.001\t Gamma Value: 0.001\t Sigma Value: 0.0\t Average Accuracy: 85.88%\n",
      "torch.Size([32, 3, 3, 3])\n",
      "P Value: 0.001\t Gamma Value: 0.001\t Sigma Value: 0.0625\t Average Accuracy: 85.12%\n",
      "torch.Size([32, 3, 3, 3])\n",
      "P Value: 0.001\t Gamma Value: 0.001\t Sigma Value: 0.125\t Average Accuracy: 85.33%\n",
      "torch.Size([32, 3, 3, 3])\n",
      "P Value: 0.001\t Gamma Value: 0.001\t Sigma Value: 0.1875\t Average Accuracy: 80.94%\n",
      "torch.Size([32, 3, 3, 3])\n",
      "P Value: 0.001\t Gamma Value: 0.001\t Sigma Value: 0.25\t Average Accuracy: 85.88%\n",
      "torch.Size([32, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "## Mask\n",
    "layer_names = ['layer1']\n",
    "perturbations = 1\n",
    "p_values = np.linspace(1e-3, 0.4, 5)\n",
    "gamma_values = np.linspace(1e-3, 0.99, 5)\n",
    "sigma = np.linspace(0, 0.25, 5)\n",
    "\n",
    "mask_noise_plots_brevitas(perturbations, layer_names, p_values, gamma_values, model, device, sigma, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ber_noise_plot_brevitas(num_perturbations, layer_names, ber_vector, model, device):\n",
    "    \n",
    "    if not os.path.exists(\"noise_plots_brevitas/ber_noise/\"):\n",
    "        os.makedirs(\"noise_plots_brevitas/ber_noise/\")\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    \n",
    "    all_test_accs = []\n",
    "\n",
    "    for layer in layer_names:\n",
    "        test_accs = []\n",
    "        \n",
    "        for ber in ber_vector:\n",
    "            noisy_models = add_digital_noise_to_model_brevitas(model, [layer], ber, num_perturbations)\n",
    "            \n",
    "            accuracies = []\n",
    "            \n",
    "            for noisy_model in noisy_models:\n",
    "                \n",
    "                noisy_model.to(device)\n",
    "                accuracies.append(test(noisy_model, test_quantized_loader, device))\n",
    "                \n",
    "            avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "            \n",
    "            test_accs.append(avg_accuracy)\n",
    "            \n",
    "            print(\"BER Value: {}\\tAverage Accuracy: {}\".format(ber, avg_accuracy))\n",
    "            \n",
    "        all_test_accs.append(test_accs)\n",
    "        \n",
    "        plt.plot(ber_vector, test_accs,\n",
    "                 label='{} Accuracy at Different Perturbation Levels'.format(layer))\n",
    "        \n",
    "        plt.xlabel('Standard Deviation')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('Effect of Noise on Test Accuracy')\n",
    "        plt.legend()\n",
    "        plt.savefig(\"noise_plots_brevitas/ber_noise/{}.png\".format(layer))\n",
    "        plt.clf()\n",
    "        print('Done with Plot {}'.format(layer))\n",
    "        \n",
    "    avg_test_accs = [sum(x) / len(x) for x in zip(*all_test_accs)]\n",
    "    \n",
    "    plt.plot(ber_vector, avg_test_accs, label='Average',\n",
    "             linewidth=3, linestyle='--', color=\"black\")\n",
    "    \n",
    "    plt.xlabel('BER Value')\n",
    "    \n",
    "    plt.ylabel('Test Accuracy')\n",
    "    \n",
    "    plt.title('Effect of BER Noise on Test Accuracy (Average)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(\"noise_plots_brevitas/ber_noise/average.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fd9d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Digital Noise with BER\n",
    "layer_names = ['layer1', 'layer2', 'layer3', 'layer4', 'layer5', 'fc1', 'fc2']\n",
    "ber_vals = [1e-6, 1e-5, 1e-4, 1e-3, 0.005, 0.01]\n",
    "perturbations = 3\n",
    "\n",
    "#ber_noise_plot_brevitas(perturbations, layer_names, ber_vals, model, device, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d701cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Gaussian Noise\n",
    "sigma_vector = np.linspace(0, 0.4, 9)\n",
    "\n",
    "gaussian_noise_plots_brevitas(3, layer_names, sigma_vector, model, device, test_quantized_loader, 0)\n",
    "gaussian_noise_plots_brevitas(3, layer_names, sigma_vector, model, device, test_quantized_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91895f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
