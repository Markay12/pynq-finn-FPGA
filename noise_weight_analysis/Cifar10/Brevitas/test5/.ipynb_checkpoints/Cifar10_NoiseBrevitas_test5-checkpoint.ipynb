{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac8d2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "from unicodedata import decimal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# add imports for randomness\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "# Brevitas imports\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.quant import Int32Bias\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For adaptive learning rate import\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "## Imports from utils file for my defined noise functions\n",
    "import sys\n",
    "#sys.path.append('C:/Users/ashin/source/repos/Cifar10_Pytorch_NoiseAnalysis/Cifar10_Pytorch_NoiseAnalysis/pynq-finn-FPGA/noise_weight_analysis/utils/')\n",
    "sys.path.append('/opt/finn/pynq-finn-FPGA/noise_weight_analysis/utils')\n",
    "\n",
    "from noise_functions import random_clust_mask, add_mask_to_model_brevitas, mask_noise_plots_brevitas, add_digital_noise, add_digital_noise_to_model_brevitas, ber_noise_plot_brevitas, add_gaussian_noise, add_gaussian_noise_to_model_brevitas, gaussian_noise_plots_brevitas, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29bdbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf08b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 730, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 728, in rmtree\n",
      "    os.rmdir(path)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-003_6enf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "50000\n",
      "Samples in each set: train = 50000, test = 391\n",
      "Shape of one input sample: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define data augmentation transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "# Use the validation transform for the validation dataset\n",
    "val_set =torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=val_transform)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "a = next(iter(train_loader))\n",
    "print(a[0].size())\n",
    "print(len(train_set))\n",
    "\n",
    "print(\"Samples in each set: train = %d, test = %s\" % (len(train_set), len(train_loader))) \n",
    "print(\"Shape of one input sample: \" +  str(train_set[0][0].shape))\n",
    "\n",
    "## Data Loader\n",
    "#\n",
    "# Using PyTorch dataloader we can create a convenient iterator over the dataset that returns batches of data, rather than requiring manual batch creation.\n",
    "\n",
    "# set batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Create a DataLoader for a training dataset with a batch size of 1000\n",
    "train_quantized_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "test_quantized_loader = DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d91be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Shape:\n",
      "-------------------------\n",
      "Input shape for 1 batch: torch.Size([128, 3, 32, 32])\n",
      "Label shape for 1 batch: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "print(\"\\nDataset Shape:\\n-------------------------\")\n",
    "for x, y in train_loader:\n",
    "    print(\"Input shape for 1 batch: \" + str(x.shape))\n",
    "    print(\"Label shape for 1 batch: \" + str(y.shape))\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a33c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10CNN, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer1 = qnn.QuantConv2d(3, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer2 = qnn.QuantConv2d(32, 32, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer3 = qnn.QuantConv2d(32, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer4 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.layer5 = qnn.QuantConv2d(64, 64, 3, padding=1, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu5 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(64 * 8 * 8, 512, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "        self.relu6 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "\n",
    "        self.fc2 = qnn.QuantLinear(512, 10, bias=True, weight_bit_width=4, bias_quant=Int32Bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant_inp(x)\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.relu2(self.layer2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu3(self.layer3(x))\n",
    "        x = self.relu4(self.layer4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.relu5(self.layer5(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.relu6(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66bec354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import testing\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Initialize the model, optimizer, and criterion\n",
    "model = CIFAR10CNN().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 80\n",
    "best_test_accuracy = 0\n",
    "patience = 8\n",
    "no_improvement_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a8babc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/391], Loss: 1.7581\n",
      "Epoch [1/80], Step [200/391], Loss: 1.7782\n",
      "Epoch [1/80], Step [300/391], Loss: 1.5384\n",
      "Epoch [1/80], Test Accuracy: 47.48%, Precision: 0.50, Recall: 0.47, F1 score: 0.46\n",
      "Epoch [2/80], Step [100/391], Loss: 1.3843\n",
      "Epoch [2/80], Step [200/391], Loss: 1.4895\n",
      "Epoch [2/80], Step [300/391], Loss: 1.2748\n",
      "Epoch [2/80], Test Accuracy: 54.13%, Precision: 0.56, Recall: 0.54, F1 score: 0.54\n",
      "Epoch [3/80], Step [100/391], Loss: 1.1483\n",
      "Epoch [3/80], Step [200/391], Loss: 1.0081\n",
      "Epoch [3/80], Step [300/391], Loss: 1.1365\n",
      "Epoch [3/80], Test Accuracy: 62.90%, Precision: 0.63, Recall: 0.63, F1 score: 0.62\n",
      "Epoch [4/80], Step [100/391], Loss: 0.9155\n",
      "Epoch [4/80], Step [200/391], Loss: 1.0246\n",
      "Epoch [4/80], Step [300/391], Loss: 0.9774\n",
      "Epoch [4/80], Test Accuracy: 68.16%, Precision: 0.69, Recall: 0.68, F1 score: 0.68\n",
      "Epoch [5/80], Step [100/391], Loss: 0.8218\n",
      "Epoch [5/80], Step [200/391], Loss: 1.0314\n",
      "Epoch [5/80], Step [300/391], Loss: 0.6680\n",
      "Epoch [5/80], Test Accuracy: 71.94%, Precision: 0.72, Recall: 0.72, F1 score: 0.72\n",
      "Epoch [6/80], Step [100/391], Loss: 0.8016\n",
      "Epoch [6/80], Step [200/391], Loss: 0.7677\n",
      "Epoch [6/80], Step [300/391], Loss: 0.7405\n",
      "Epoch [6/80], Test Accuracy: 72.89%, Precision: 0.74, Recall: 0.73, F1 score: 0.73\n",
      "Epoch [7/80], Step [100/391], Loss: 0.9120\n",
      "Epoch [7/80], Step [200/391], Loss: 0.8253\n",
      "Epoch [7/80], Step [300/391], Loss: 0.7368\n",
      "Epoch [7/80], Test Accuracy: 75.04%, Precision: 0.75, Recall: 0.75, F1 score: 0.75\n",
      "Epoch [8/80], Step [100/391], Loss: 0.8806\n",
      "Epoch [8/80], Step [200/391], Loss: 0.5637\n",
      "Epoch [8/80], Step [300/391], Loss: 0.7883\n",
      "Epoch [8/80], Test Accuracy: 77.41%, Precision: 0.77, Recall: 0.77, F1 score: 0.77\n",
      "Epoch [9/80], Step [100/391], Loss: 0.5704\n",
      "Epoch [9/80], Step [200/391], Loss: 0.7288\n",
      "Epoch [9/80], Step [300/391], Loss: 0.7240\n",
      "Epoch [9/80], Test Accuracy: 76.69%, Precision: 0.77, Recall: 0.77, F1 score: 0.77\n",
      "Epoch [10/80], Step [100/391], Loss: 0.6490\n",
      "Epoch [10/80], Step [200/391], Loss: 0.6672\n",
      "Epoch [10/80], Step [300/391], Loss: 0.5111\n",
      "Epoch [10/80], Test Accuracy: 77.79%, Precision: 0.78, Recall: 0.78, F1 score: 0.77\n",
      "Epoch [11/80], Step [100/391], Loss: 0.7193\n",
      "Epoch [11/80], Step [200/391], Loss: 0.6091\n",
      "Epoch [11/80], Step [300/391], Loss: 0.6770\n",
      "Epoch [11/80], Test Accuracy: 78.18%, Precision: 0.78, Recall: 0.78, F1 score: 0.78\n",
      "Epoch [12/80], Step [100/391], Loss: 0.4757\n",
      "Epoch [12/80], Step [200/391], Loss: 0.5535\n",
      "Epoch [12/80], Step [300/391], Loss: 0.5280\n",
      "Epoch [12/80], Test Accuracy: 78.66%, Precision: 0.79, Recall: 0.79, F1 score: 0.79\n",
      "Epoch [13/80], Step [100/391], Loss: 0.5775\n",
      "Epoch [13/80], Step [200/391], Loss: 0.6216\n",
      "Epoch [13/80], Step [300/391], Loss: 0.6164\n",
      "Epoch [13/80], Test Accuracy: 79.93%, Precision: 0.80, Recall: 0.80, F1 score: 0.80\n",
      "Epoch [14/80], Step [100/391], Loss: 0.6831\n",
      "Epoch [14/80], Step [200/391], Loss: 0.4670\n",
      "Epoch [14/80], Step [300/391], Loss: 0.6208\n",
      "Epoch [14/80], Test Accuracy: 78.68%, Precision: 0.80, Recall: 0.79, F1 score: 0.79\n",
      "Epoch [15/80], Step [100/391], Loss: 0.5503\n",
      "Epoch [15/80], Step [200/391], Loss: 0.6811\n",
      "Epoch [15/80], Step [300/391], Loss: 0.6160\n",
      "Epoch [15/80], Test Accuracy: 80.47%, Precision: 0.81, Recall: 0.80, F1 score: 0.81\n",
      "Epoch [16/80], Step [100/391], Loss: 0.4160\n",
      "Epoch [16/80], Step [200/391], Loss: 0.4325\n",
      "Epoch [16/80], Step [300/391], Loss: 0.5355\n",
      "Epoch [16/80], Test Accuracy: 80.83%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [17/80], Step [100/391], Loss: 0.6051\n",
      "Epoch [17/80], Step [200/391], Loss: 0.6533\n",
      "Epoch [17/80], Step [300/391], Loss: 0.6307\n",
      "Epoch [17/80], Test Accuracy: 80.44%, Precision: 0.81, Recall: 0.80, F1 score: 0.80\n",
      "Epoch [18/80], Step [100/391], Loss: 0.3363\n",
      "Epoch [18/80], Step [200/391], Loss: 0.5399\n",
      "Epoch [18/80], Step [300/391], Loss: 0.3899\n",
      "Epoch [18/80], Test Accuracy: 80.67%, Precision: 0.81, Recall: 0.81, F1 score: 0.81\n",
      "Epoch [19/80], Step [100/391], Loss: 0.5653\n",
      "Epoch [19/80], Step [200/391], Loss: 0.5613\n",
      "Epoch [19/80], Step [300/391], Loss: 0.4430\n",
      "Epoch [19/80], Test Accuracy: 82.07%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [20/80], Step [100/391], Loss: 0.6895\n",
      "Epoch [20/80], Step [200/391], Loss: 0.4871\n",
      "Epoch [20/80], Step [300/391], Loss: 0.5984\n",
      "Epoch [20/80], Test Accuracy: 82.19%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [21/80], Step [100/391], Loss: 0.5234\n",
      "Epoch [21/80], Step [200/391], Loss: 0.3165\n",
      "Epoch [21/80], Step [300/391], Loss: 0.3694\n",
      "Epoch [21/80], Test Accuracy: 81.84%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [22/80], Step [100/391], Loss: 0.4516\n",
      "Epoch [22/80], Step [200/391], Loss: 0.4931\n",
      "Epoch [22/80], Step [300/391], Loss: 0.5905\n",
      "Epoch [22/80], Test Accuracy: 82.05%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [23/80], Step [100/391], Loss: 0.5298\n",
      "Epoch [23/80], Step [200/391], Loss: 0.4648\n",
      "Epoch [23/80], Step [300/391], Loss: 0.3971\n",
      "Epoch [23/80], Test Accuracy: 81.58%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [24/80], Step [100/391], Loss: 0.5040\n",
      "Epoch [24/80], Step [200/391], Loss: 0.6251\n",
      "Epoch [24/80], Step [300/391], Loss: 0.4162\n",
      "Epoch [24/80], Test Accuracy: 82.70%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [25/80], Step [100/391], Loss: 0.3257\n",
      "Epoch [25/80], Step [200/391], Loss: 0.5446\n",
      "Epoch [25/80], Step [300/391], Loss: 0.4239\n",
      "Epoch [25/80], Test Accuracy: 82.68%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [26/80], Step [100/391], Loss: 0.3806\n",
      "Epoch [26/80], Step [200/391], Loss: 0.3493\n",
      "Epoch [26/80], Step [300/391], Loss: 0.3351\n",
      "Epoch [26/80], Test Accuracy: 81.99%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [27/80], Step [100/391], Loss: 0.4180\n",
      "Epoch [27/80], Step [200/391], Loss: 0.3398\n",
      "Epoch [27/80], Step [300/391], Loss: 0.5127\n",
      "Epoch [27/80], Test Accuracy: 82.03%, Precision: 0.82, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [28/80], Step [100/391], Loss: 0.4848\n",
      "Epoch [28/80], Step [200/391], Loss: 0.4115\n",
      "Epoch [28/80], Step [300/391], Loss: 0.4190\n",
      "Epoch [28/80], Test Accuracy: 83.08%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [29/80], Step [100/391], Loss: 0.4946\n",
      "Epoch [29/80], Step [200/391], Loss: 0.3391\n",
      "Epoch [29/80], Step [300/391], Loss: 0.4464\n",
      "Epoch [29/80], Test Accuracy: 82.67%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [30/80], Step [100/391], Loss: 0.3956\n",
      "Epoch [30/80], Step [200/391], Loss: 0.3198\n",
      "Epoch [30/80], Step [300/391], Loss: 0.3413\n",
      "Epoch [30/80], Test Accuracy: 82.78%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [31/80], Step [100/391], Loss: 0.3399\n",
      "Epoch [31/80], Step [200/391], Loss: 0.4714\n",
      "Epoch [31/80], Step [300/391], Loss: 0.3434\n",
      "Epoch [31/80], Test Accuracy: 82.67%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [32/80], Step [100/391], Loss: 0.4009\n",
      "Epoch [32/80], Step [200/391], Loss: 0.4383\n",
      "Epoch [32/80], Step [300/391], Loss: 0.3666\n",
      "Epoch [32/80], Test Accuracy: 82.43%, Precision: 0.83, Recall: 0.82, F1 score: 0.82\n",
      "Epoch [33/80], Step [100/391], Loss: 0.3740\n",
      "Epoch [33/80], Step [200/391], Loss: 0.2418\n",
      "Epoch [33/80], Step [300/391], Loss: 0.4185\n",
      "Epoch [33/80], Test Accuracy: 82.78%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [34/80], Step [100/391], Loss: 0.4220\n",
      "Epoch [34/80], Step [200/391], Loss: 0.3278\n",
      "Epoch [34/80], Step [300/391], Loss: 0.3743\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [34/80], Test Accuracy: 82.60%, Precision: 0.83, Recall: 0.83, F1 score: 0.83\n",
      "Epoch [35/80], Step [100/391], Loss: 0.3853\n",
      "Epoch [35/80], Step [200/391], Loss: 0.4061\n",
      "Epoch [35/80], Step [300/391], Loss: 0.3725\n",
      "Epoch [35/80], Test Accuracy: 84.78%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [36/80], Step [100/391], Loss: 0.1976\n",
      "Epoch [36/80], Step [200/391], Loss: 0.2598\n",
      "Epoch [36/80], Step [300/391], Loss: 0.2799\n",
      "Epoch [36/80], Test Accuracy: 85.00%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [37/80], Step [100/391], Loss: 0.3504\n",
      "Epoch [37/80], Step [200/391], Loss: 0.3695\n",
      "Epoch [37/80], Step [300/391], Loss: 0.2786\n",
      "Epoch [37/80], Test Accuracy: 85.08%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [38/80], Step [100/391], Loss: 0.4122\n",
      "Epoch [38/80], Step [200/391], Loss: 0.3063\n",
      "Epoch [38/80], Step [300/391], Loss: 0.2919\n",
      "Epoch [38/80], Test Accuracy: 84.95%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/80], Step [100/391], Loss: 0.2842\n",
      "Epoch [39/80], Step [200/391], Loss: 0.2820\n",
      "Epoch [39/80], Step [300/391], Loss: 0.2912\n",
      "Epoch [39/80], Test Accuracy: 85.10%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [40/80], Step [100/391], Loss: 0.3431\n",
      "Epoch [40/80], Step [200/391], Loss: 0.4236\n",
      "Epoch [40/80], Step [300/391], Loss: 0.3841\n",
      "Epoch [40/80], Test Accuracy: 85.27%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [41/80], Step [100/391], Loss: 0.2618\n",
      "Epoch [41/80], Step [200/391], Loss: 0.4188\n",
      "Epoch [41/80], Step [300/391], Loss: 0.3041\n",
      "Epoch [41/80], Test Accuracy: 85.17%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [42/80], Step [100/391], Loss: 0.2462\n",
      "Epoch [42/80], Step [200/391], Loss: 0.3617\n",
      "Epoch [42/80], Step [300/391], Loss: 0.2576\n",
      "Epoch [42/80], Test Accuracy: 85.42%, Precision: 0.86, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [43/80], Step [100/391], Loss: 0.2355\n",
      "Epoch [43/80], Step [200/391], Loss: 0.3305\n",
      "Epoch [43/80], Step [300/391], Loss: 0.2448\n",
      "Epoch [43/80], Test Accuracy: 85.16%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [44/80], Step [100/391], Loss: 0.2337\n",
      "Epoch [44/80], Step [200/391], Loss: 0.2942\n",
      "Epoch [44/80], Step [300/391], Loss: 0.2459\n",
      "Epoch [44/80], Test Accuracy: 85.37%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [45/80], Step [100/391], Loss: 0.2364\n",
      "Epoch [45/80], Step [200/391], Loss: 0.2288\n",
      "Epoch [45/80], Step [300/391], Loss: 0.2341\n",
      "Epoch [45/80], Test Accuracy: 85.21%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [46/80], Step [100/391], Loss: 0.2942\n",
      "Epoch [46/80], Step [200/391], Loss: 0.2354\n",
      "Epoch [46/80], Step [300/391], Loss: 0.1502\n",
      "Epoch [46/80], Test Accuracy: 85.32%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [47/80], Step [100/391], Loss: 0.2755\n",
      "Epoch [47/80], Step [200/391], Loss: 0.2466\n",
      "Epoch [47/80], Step [300/391], Loss: 0.2457\n",
      "Epoch [47/80], Test Accuracy: 85.40%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [48/80], Step [100/391], Loss: 0.2227\n",
      "Epoch [48/80], Step [200/391], Loss: 0.2838\n",
      "Epoch [48/80], Step [300/391], Loss: 0.2887\n",
      "Epoch [48/80], Test Accuracy: 85.36%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [49/80], Step [100/391], Loss: 0.2738\n",
      "Epoch [49/80], Step [200/391], Loss: 0.1826\n",
      "Epoch [49/80], Step [300/391], Loss: 0.3098\n",
      "Epoch [49/80], Test Accuracy: 85.24%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [50/80], Step [100/391], Loss: 0.3175\n",
      "Epoch [50/80], Step [200/391], Loss: 0.2611\n",
      "Epoch [50/80], Step [300/391], Loss: 0.2996\n",
      "Epoch [50/80], Test Accuracy: 85.45%, Precision: 0.86, Recall: 0.85, F1 score: 0.86\n",
      "Epoch [51/80], Step [100/391], Loss: 0.2363\n",
      "Epoch [51/80], Step [200/391], Loss: 0.2093\n",
      "Epoch [51/80], Step [300/391], Loss: 0.3499\n",
      "Epoch [51/80], Test Accuracy: 85.21%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [52/80], Step [100/391], Loss: 0.2732\n",
      "Epoch [52/80], Step [200/391], Loss: 0.2648\n",
      "Epoch [52/80], Step [300/391], Loss: 0.3473\n",
      "Epoch [52/80], Test Accuracy: 85.18%, Precision: 0.85, Recall: 0.85, F1 score: 0.85\n",
      "Epoch [53/80], Step [100/391], Loss: 0.1872\n",
      "Epoch [53/80], Step [200/391], Loss: 0.2043\n",
      "Epoch [53/80], Step [300/391], Loss: 0.2742\n",
      "Epoch 00053: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [53/80], Test Accuracy: 85.65%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [54/80], Step [100/391], Loss: 0.2709\n",
      "Epoch [54/80], Step [200/391], Loss: 0.2192\n",
      "Epoch [54/80], Step [300/391], Loss: 0.1595\n",
      "Epoch [54/80], Test Accuracy: 85.62%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [55/80], Step [100/391], Loss: 0.3372\n",
      "Epoch [55/80], Step [200/391], Loss: 0.2548\n",
      "Epoch [55/80], Step [300/391], Loss: 0.2574\n",
      "Epoch [55/80], Test Accuracy: 85.53%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [56/80], Step [100/391], Loss: 0.2412\n",
      "Epoch [56/80], Step [200/391], Loss: 0.2721\n",
      "Epoch [56/80], Step [300/391], Loss: 0.1675\n",
      "Epoch [56/80], Test Accuracy: 85.88%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [57/80], Step [100/391], Loss: 0.3224\n",
      "Epoch [57/80], Step [200/391], Loss: 0.1434\n",
      "Epoch [57/80], Step [300/391], Loss: 0.3064\n",
      "Epoch [57/80], Test Accuracy: 85.81%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [58/80], Step [100/391], Loss: 0.1886\n",
      "Epoch [58/80], Step [200/391], Loss: 0.2931\n",
      "Epoch [58/80], Step [300/391], Loss: 0.1887\n",
      "Epoch [58/80], Test Accuracy: 85.60%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [59/80], Step [100/391], Loss: 0.2622\n",
      "Epoch [59/80], Step [200/391], Loss: 0.1352\n",
      "Epoch [59/80], Step [300/391], Loss: 0.1795\n",
      "Epoch [59/80], Test Accuracy: 85.80%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [60/80], Step [100/391], Loss: 0.2411\n",
      "Epoch [60/80], Step [200/391], Loss: 0.3285\n",
      "Epoch [60/80], Step [300/391], Loss: 0.4192\n",
      "Epoch [60/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [61/80], Step [100/391], Loss: 0.2320\n",
      "Epoch [61/80], Step [200/391], Loss: 0.2958\n",
      "Epoch [61/80], Step [300/391], Loss: 0.2420\n",
      "Epoch [61/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [62/80], Step [100/391], Loss: 0.3244\n",
      "Epoch [62/80], Step [200/391], Loss: 0.2040\n",
      "Epoch [62/80], Step [300/391], Loss: 0.1512\n",
      "Epoch [62/80], Test Accuracy: 85.73%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [63/80], Step [100/391], Loss: 0.2197\n",
      "Epoch [63/80], Step [200/391], Loss: 0.2291\n",
      "Epoch [63/80], Step [300/391], Loss: 0.2767\n",
      "Epoch [63/80], Test Accuracy: 85.95%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [64/80], Step [100/391], Loss: 0.2288\n",
      "Epoch [64/80], Step [200/391], Loss: 0.2318\n",
      "Epoch [64/80], Step [300/391], Loss: 0.1853\n",
      "Epoch [64/80], Test Accuracy: 85.94%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [65/80], Step [100/391], Loss: 0.2293\n",
      "Epoch [65/80], Step [200/391], Loss: 0.2891\n",
      "Epoch [65/80], Step [300/391], Loss: 0.1683\n",
      "Epoch [65/80], Test Accuracy: 85.65%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [66/80], Step [100/391], Loss: 0.1923\n",
      "Epoch [66/80], Step [200/391], Loss: 0.1138\n",
      "Epoch [66/80], Step [300/391], Loss: 0.2055\n",
      "Epoch [66/80], Test Accuracy: 85.64%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [67/80], Step [100/391], Loss: 0.1742\n",
      "Epoch [67/80], Step [200/391], Loss: 0.3278\n",
      "Epoch [67/80], Step [300/391], Loss: 0.2256\n",
      "Epoch [67/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [68/80], Step [100/391], Loss: 0.1497\n",
      "Epoch [68/80], Step [200/391], Loss: 0.1958\n",
      "Epoch [68/80], Step [300/391], Loss: 0.2030\n",
      "Epoch 00068: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch [68/80], Test Accuracy: 85.68%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [69/80], Step [100/391], Loss: 0.2898\n",
      "Epoch [69/80], Step [200/391], Loss: 0.2270\n",
      "Epoch [69/80], Step [300/391], Loss: 0.2735\n",
      "Epoch [69/80], Test Accuracy: 85.89%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [70/80], Step [100/391], Loss: 0.2381\n",
      "Epoch [70/80], Step [200/391], Loss: 0.2529\n",
      "Epoch [70/80], Step [300/391], Loss: 0.1969\n",
      "Epoch [70/80], Test Accuracy: 85.99%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [71/80], Step [100/391], Loss: 0.1510\n",
      "Epoch [71/80], Step [200/391], Loss: 0.2830\n",
      "Epoch [71/80], Step [300/391], Loss: 0.3289\n",
      "Epoch [71/80], Test Accuracy: 85.90%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [72/80], Step [100/391], Loss: 0.2035\n",
      "Epoch [72/80], Step [200/391], Loss: 0.2620\n",
      "Epoch [72/80], Step [300/391], Loss: 0.2721\n",
      "Epoch [72/80], Test Accuracy: 85.91%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [73/80], Step [100/391], Loss: 0.2259\n",
      "Epoch [73/80], Step [200/391], Loss: 0.3350\n",
      "Epoch [73/80], Step [300/391], Loss: 0.3486\n",
      "Epoch [73/80], Test Accuracy: 85.99%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [74/80], Step [100/391], Loss: 0.2075\n",
      "Epoch [74/80], Step [200/391], Loss: 0.2492\n",
      "Epoch [74/80], Step [300/391], Loss: 0.2433\n",
      "Epoch [74/80], Test Accuracy: 85.71%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [75/80], Step [100/391], Loss: 0.2900\n",
      "Epoch [75/80], Step [200/391], Loss: 0.2705\n",
      "Epoch [75/80], Step [300/391], Loss: 0.1964\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch [75/80], Test Accuracy: 85.90%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [76/80], Step [100/391], Loss: 0.1782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/80], Step [200/391], Loss: 0.2292\n",
      "Epoch [76/80], Step [300/391], Loss: 0.2834\n",
      "Epoch [76/80], Test Accuracy: 85.79%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [77/80], Step [100/391], Loss: 0.2393\n",
      "Epoch [77/80], Step [200/391], Loss: 0.2057\n",
      "Epoch [77/80], Step [300/391], Loss: 0.3019\n",
      "Epoch [77/80], Test Accuracy: 85.83%, Precision: 0.86, Recall: 0.86, F1 score: 0.86\n",
      "Epoch [78/80], Step [100/391], Loss: 0.2675\n",
      "Epoch [78/80], Step [200/391], Loss: 0.2053\n",
      "Epoch [78/80], Step [300/391], Loss: 0.2695\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # training phase\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "    \n",
    "    # Initialize the validation loss\n",
    "    val_loss = 0\n",
    "    \n",
    "    # testing phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()  # accumulate the validation loss\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "        val_loss /= len(val_loader)  # calculate the average validation loss\n",
    "    \n",
    "        # Update the learning rate using the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        test_accuracy = 100 * correct / total\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "        \n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), 'test5_model.pth')\n",
    "            no_improvement_counter = 0\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        print('Epoch [{}/{}], Test Accuracy: {:.2f}%, Precision: {:.2f}, Recall: {:.2f}, F1 score: {:.2f}'.format(epoch+1, num_epochs, test_accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055fe5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final best test accuracy is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Print the best test accuracy\n",
    "print(\"The final best test accuracy is: {:.2f}%\".format(best_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966f95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b94d7e",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d3d7f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_385732/3428216673.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_test_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbest_test_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     torch.save({\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "if test_accuracy > best_test_accuracy:\n",
    "    best_test_accuracy = test_accuracy\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_test_accuracy': best_test_accuracy,\n",
    "        'epoch': epoch\n",
    "        }, 'test5_model.pth')\n",
    "    no_improvement_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3573bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of your neural network model\n",
    "model = CIFAR10CNN().to(device)\n",
    "\n",
    "# Load the saved state dictionary from file\n",
    "state_dict = torch.load('test5_model.pth')\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359fd9d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BER Value: 1e-06\tAverage Accuracy: 85.98\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.98\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.79\n",
      "BER Value: 0.001\tAverage Accuracy: 85.10800000000002\n",
      "BER Value: 0.01\tAverage Accuracy: 78.877\n",
      "BER Value: 0.1\tAverage Accuracy: 51.059000000000005\n",
      "Done with Plot layer1\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.929\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.95500000000001\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.125\n",
      "BER Value: 0.001\tAverage Accuracy: 83.817\n",
      "BER Value: 0.01\tAverage Accuracy: 78.369\n",
      "BER Value: 0.1\tAverage Accuracy: 31.75\n",
      "Done with Plot layer2\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.98\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.76700000000001\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.383\n",
      "BER Value: 0.001\tAverage Accuracy: 84.94800000000001\n",
      "BER Value: 0.01\tAverage Accuracy: 82.85\n",
      "BER Value: 0.1\tAverage Accuracy: 53.922000000000004\n",
      "Done with Plot layer3\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.98\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.58500000000001\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.28899999999999\n",
      "BER Value: 0.001\tAverage Accuracy: 84.999\n",
      "BER Value: 0.01\tAverage Accuracy: 83.391\n",
      "BER Value: 0.1\tAverage Accuracy: 63.576\n",
      "Done with Plot layer4\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.98\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.504\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.05499999999999\n",
      "BER Value: 0.001\tAverage Accuracy: 83.796\n",
      "BER Value: 0.01\tAverage Accuracy: 81.404\n",
      "BER Value: 0.1\tAverage Accuracy: 57.971999999999994\n",
      "Done with Plot layer5\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.83399999999999\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.539\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.46300000000001\n",
      "BER Value: 0.001\tAverage Accuracy: 85.48300000000002\n",
      "BER Value: 0.01\tAverage Accuracy: 84.91900000000001\n",
      "BER Value: 0.1\tAverage Accuracy: 79.586\n",
      "Done with Plot fc1\n",
      "BER Value: 1e-06\tAverage Accuracy: 85.98\n",
      "BER Value: 1e-05\tAverage Accuracy: 85.98\n",
      "BER Value: 0.0001\tAverage Accuracy: 85.73100000000002\n",
      "BER Value: 0.001\tAverage Accuracy: 84.89000000000001\n",
      "BER Value: 0.01\tAverage Accuracy: 81.73400000000001\n",
      "BER Value: 0.1\tAverage Accuracy: 63.766\n",
      "Done with Plot fc2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Digital Noise with BER\n",
    "layer_names = ['layer1', 'layer2', 'layer3', 'layer4', 'layer5', 'fc1', 'fc2']\n",
    "ber_vals = [1e-6, 1e-5, 1e-4, 1e-3, 0.01, 0.1]\n",
    "perturbations = 10\n",
    "\n",
    "ber_noise_plot_brevitas(perturbations, layer_names, ber_vals, model, device, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabe2b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma Value: 0.0, Average Accuracy: 85.98%\n",
      "Sigma Value: 0.02, Average Accuracy: 84.498%\n",
      "Sigma Value: 0.04, Average Accuracy: 82.411%\n",
      "Sigma Value: 0.06, Average Accuracy: 77.66300000000001%\n",
      "Sigma Value: 0.08, Average Accuracy: 70.555%\n",
      "Sigma Value: 0.1, Average Accuracy: 65.511%\n",
      "Sigma Value: 0.12, Average Accuracy: 56.013999999999996%\n",
      "Sigma Value: 0.14, Average Accuracy: 52.23299999999999%\n",
      "Sigma Value: 0.16, Average Accuracy: 39.106%\n",
      "Sigma Value: 0.18, Average Accuracy: 34.62%\n",
      "Sigma Value: 0.2, Average Accuracy: 31.430999999999994%\n",
      "Done with Plot layer1\n",
      "Sigma Value: 0.0, Average Accuracy: 85.98%\n",
      "Sigma Value: 0.02, Average Accuracy: 84.95700000000001%\n",
      "Sigma Value: 0.04, Average Accuracy: 82.84%\n",
      "Sigma Value: 0.06, Average Accuracy: 77.07699999999998%\n",
      "Sigma Value: 0.08, Average Accuracy: 70.17299999999999%\n",
      "Sigma Value: 0.1, Average Accuracy: 61.31699999999999%\n",
      "Sigma Value: 0.12, Average Accuracy: 47.024%\n",
      "Sigma Value: 0.14, Average Accuracy: 42.747%\n",
      "Sigma Value: 0.16, Average Accuracy: 35.36%\n",
      "Sigma Value: 0.18, Average Accuracy: 30.580000000000002%\n",
      "Sigma Value: 0.2, Average Accuracy: 27.077999999999996%\n",
      "Done with Plot layer2\n",
      "Sigma Value: 0.0, Average Accuracy: 85.98%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.21499999999999%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.155%\n",
      "Sigma Value: 0.06, Average Accuracy: 81.785%\n",
      "Sigma Value: 0.08, Average Accuracy: 78.82399999999998%\n",
      "Sigma Value: 0.1, Average Accuracy: 75.939%\n",
      "Sigma Value: 0.12, Average Accuracy: 70.084%\n",
      "Sigma Value: 0.14, Average Accuracy: 63.858000000000004%\n",
      "Sigma Value: 0.16, Average Accuracy: 53.766%\n",
      "Sigma Value: 0.18, Average Accuracy: 56.144000000000005%\n",
      "Sigma Value: 0.2, Average Accuracy: 44.04699999999999%\n",
      "Done with Plot layer3\n",
      "Sigma Value: 0.0, Average Accuracy: 85.98%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.527%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.59400000000001%\n",
      "Sigma Value: 0.06, Average Accuracy: 83.284%\n",
      "Sigma Value: 0.08, Average Accuracy: 81.42600000000002%\n",
      "Sigma Value: 0.1, Average Accuracy: 77.66999999999999%\n",
      "Sigma Value: 0.12, Average Accuracy: 73.75399999999999%\n",
      "Sigma Value: 0.14, Average Accuracy: 72.418%\n",
      "Sigma Value: 0.16, Average Accuracy: 65.78099999999999%\n",
      "Sigma Value: 0.18, Average Accuracy: 62.016%\n",
      "Sigma Value: 0.2, Average Accuracy: 51.86900000000001%\n",
      "Done with Plot layer4\n",
      "Sigma Value: 0.0, Average Accuracy: 85.98%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.43700000000001%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.31299999999999%\n",
      "Sigma Value: 0.06, Average Accuracy: 82.64399999999999%\n",
      "Sigma Value: 0.08, Average Accuracy: 80.178%\n",
      "Sigma Value: 0.1, Average Accuracy: 76.819%\n",
      "Sigma Value: 0.12, Average Accuracy: 72.619%\n",
      "Sigma Value: 0.14, Average Accuracy: 66.292%\n",
      "Sigma Value: 0.16, Average Accuracy: 63.053%\n",
      "Sigma Value: 0.18, Average Accuracy: 58.891999999999996%\n",
      "Sigma Value: 0.2, Average Accuracy: 52.54200000000001%\n",
      "Done with Plot layer5\n",
      "Sigma Value: 0.0, Average Accuracy: 85.98%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.824%\n",
      "Sigma Value: 0.04, Average Accuracy: 85.46100000000001%\n",
      "Sigma Value: 0.06, Average Accuracy: 85.18100000000001%\n",
      "Sigma Value: 0.08, Average Accuracy: 84.61099999999999%\n",
      "Sigma Value: 0.1, Average Accuracy: 83.863%\n",
      "Sigma Value: 0.12, Average Accuracy: 83.21100000000001%\n",
      "Sigma Value: 0.14, Average Accuracy: 82.44200000000001%\n",
      "Sigma Value: 0.16, Average Accuracy: 81.208%\n",
      "Sigma Value: 0.18, Average Accuracy: 79.88500000000002%\n",
      "Sigma Value: 0.2, Average Accuracy: 78.43300000000002%\n",
      "Done with Plot fc1\n",
      "Sigma Value: 0.0, Average Accuracy: 85.98%\n",
      "Sigma Value: 0.02, Average Accuracy: 85.401%\n",
      "Sigma Value: 0.04, Average Accuracy: 84.31499999999998%\n",
      "Sigma Value: 0.06, Average Accuracy: 82.35600000000001%\n",
      "Sigma Value: 0.08, Average Accuracy: 80.78999999999999%\n",
      "Sigma Value: 0.1, Average Accuracy: 77.378%\n",
      "Sigma Value: 0.12, Average Accuracy: 73.684%\n",
      "Sigma Value: 0.14, Average Accuracy: 69.62700000000001%\n"
     ]
    }
   ],
   "source": [
    "## Gaussian Noise\n",
    "sigma_vector = np.linspace(0, 0.2, 11)\n",
    "\n",
    "gaussian_noise_plots_brevitas(perturbations, layer_names, sigma_vector, model, device, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd1970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
